{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import mne\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import entropy\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "from mne.preprocessing import ICA\n",
    "from scipy.signal import butter, filtfilt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928dce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling rate\n",
    "sfreq = 500  # Hz\n",
    "\n",
    "# Load your datasets\n",
    "with open(\"/home/donaf-strange/LAB_WORK/eeg_arithmetic_project/data/all_rest_eeg_by_participant.pkl\", 'rb') as f:\n",
    "    rest_eeg_dict = pickle.load(f)\n",
    "\n",
    "with open(\"/home/donaf-strange/LAB_WORK/eeg_arithmetic_project/data/all_task_eeg_by_participant.pkl\", 'rb') as f:\n",
    "    task_eeg_dict = pickle.load(f)\n",
    "\n",
    "# Define mapping for EEG channels (from raw names to standard ones)\n",
    "channel_mapping = {\n",
    "    \"EEG Fp1\": \"Fp1\", \"EEG Fp2\": \"Fp2\", \"EEG F3\": \"F3\", \"EEG F4\": \"F4\",\n",
    "    \"EEG F7\": \"F7\", \"EEG F8\": \"F8\", \"EEG T3\": \"T3\", \"EEG T4\": \"T4\",\n",
    "    \"EEG C3\": \"C3\", \"EEG C4\": \"C4\", \"EEG T5\": \"T5\", \"EEG T6\": \"T6\",\n",
    "    \"EEG P3\": \"P3\", \"EEG P4\": \"P4\", \"EEG O1\": \"O1\", \"EEG O2\": \"O2\",\n",
    "    \"EEG Fz\": \"Fz\", \"EEG Cz\": \"Cz\", \"EEG Pz\": \"Pz\"\n",
    "}\n",
    "\n",
    "# Preprocessing for one subject\n",
    "def preprocess_subject(raw: mne.io.Raw, subject_id=None, label='UNKNOWN', n_components=0.99):\n",
    "    raw = raw.copy()\n",
    "\n",
    "    # Rename EEG channels\n",
    "    raw.rename_channels(lambda ch: ch.replace(\"EEG \", \"\") if ch.startswith(\"EEG \") else ch)\n",
    "\n",
    "    # Keep only EEG channels\n",
    "    raw.pick(list(channel_mapping.values()))\n",
    "\n",
    "    # Set average reference\n",
    "    raw.set_eeg_reference('average', projection=False)\n",
    "\n",
    "    # ICA - fit on high-pass filtered copy only (NOT applied)\n",
    "    raw_ica = raw.copy().filter(l_freq=1.0, h_freq=None, fir_design='firwin', verbose=False)\n",
    "    ica = ICA(n_components=n_components, random_state=97, max_iter='auto')\n",
    "    ica.fit(raw_ica)\n",
    "\n",
    "    # Apply ICA to full-band raw signal\n",
    "    raw = ica.apply(raw)\n",
    "    print(f\"üìä Subject {subject_id} | {label} | ICA components used: {ica.n_components_}\")\n",
    "\n",
    "    # Frequency bands\n",
    "`    bands = {\n",
    "        #'delta': (1, 4),\n",
    "        'theta': (4, 8),\n",
    "        'alpha': (8, 12),\n",
    "        'beta': (12, 30),\n",
    "        'gamma': (30, 45),\n",
    "        'broadband': (1, 45)\n",
    "    }\n",
    "`\n",
    "\n",
    "    # Band-specific filtered data\n",
    "    band_data = {}\n",
    "    for band, (low, high) in bands.items():\n",
    "        raw_band = raw.copy().filter(l_freq=low, h_freq=high, fir_design='firwin', verbose=False)\n",
    "        band_data[band] = raw_band.get_data()\n",
    "\n",
    "    return band_data\n",
    "\n",
    "\n",
    "# Batch preprocessing\n",
    "def batch_preprocess(eeg_dict, label='UNKNOWN'):\n",
    "    preprocessed = {}\n",
    "    for subject_id, raw in eeg_dict.items():\n",
    "        try:\n",
    "            band_data = preprocess_subject(raw, subject_id=subject_id, label=label)\n",
    "            preprocessed[subject_id] = band_data\n",
    "            print(f\"‚úÖ Preprocessed {label} subject {subject_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed {label} subject {subject_id}: {e}\")\n",
    "    return preprocessed\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ed150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save outputs for Step 2\n",
    "Path(\"preprocessed\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_task = batch_preprocess(task_eeg_dict, label='TASK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d33f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"preprocessed/preprocessed_task.pkl\", \"wb\") as f:\n",
    "    pickle.dump(preprocessed_task, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_rest = batch_preprocess(rest_eeg_dict, label='REST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acda639",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"preprocessed/preprocessed_rest.pkl\", \"wb\") as f:\n",
    "    pickle.dump(preprocessed_rest, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binarize_subject_band(band_data, method='median'):\n",
    "    binary_band = {}\n",
    "\n",
    "    for band, data in band_data.items():\n",
    "        n_channels, n_samples = data.shape\n",
    "        binary = np.zeros_like(data, dtype=np.uint8)\n",
    "\n",
    "        for ch in range(n_channels):\n",
    "            if method == 'median':\n",
    "                threshold = np.median(data[ch])\n",
    "            elif method == 'quantile':\n",
    "                threshold = np.quantile(data[ch], 0.5)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported binarization method.\")\n",
    "\n",
    "            binary[ch] = (data[ch] > threshold).astype(np.uint8)\n",
    "\n",
    "        binary_band[band] = binary\n",
    "\n",
    "    return binary_band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a93f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_binarize(preprocessed_dict, label='UNKNOWN'):\n",
    "    binary_all = {}\n",
    "    for subj_id, band_data in preprocessed_dict.items():\n",
    "        try:\n",
    "            binary_all[subj_id] = binarize_subject_band(band_data)\n",
    "            print(f\"‚úÖ Binarized {label} subject {subj_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed {label} subject {subj_id}: {e}\")\n",
    "    return binary_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8899cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed filtered data\n",
    "with open(\"preprocessed/preprocessed_rest.pkl\", \"rb\") as f:\n",
    "    preprocessed_rest = pickle.load(f)\n",
    "\n",
    "with open(\"preprocessed/preprocessed_task.pkl\", \"rb\") as f:\n",
    "    preprocessed_task = pickle.load(f)\n",
    "\n",
    "# Run\n",
    "binary_rest = batch_binarize(preprocessed_rest, label='REST')\n",
    "binary_task = batch_binarize(preprocessed_task, label='TASK')\n",
    "\n",
    "# Save\n",
    "Path(\"binarized\").mkdir(exist_ok=True)\n",
    "with open(\"binarized/binary_rest.pkl\", \"wb\") as f:\n",
    "    pickle.dump(binary_rest, f)\n",
    "\n",
    "with open(\"binarized/binary_task.pkl\", \"wb\") as f:\n",
    "    pickle.dump(binary_task, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9329a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pairwise_ising(X):\n",
    "    \"\"\"\n",
    "    Fit Ising model using logistic regression (pseudo-likelihood method).\n",
    "    Input: X ‚àà {0,1}^n_samples √ó n_channels\n",
    "    Output: h ‚àà R^n, J ‚àà R^{n √ó n}\n",
    "    \"\"\"\n",
    "    X = X.copy().astype(int)  # Fix: convert from uint8 ‚Üí int\n",
    "    X[X == 0] = -1  # Convert 0 ‚Üí -1\n",
    "    n_samples, n_channels = X.shape\n",
    "\n",
    "    h = np.zeros(n_channels)\n",
    "    J = np.zeros((n_channels, n_channels))\n",
    "\n",
    "    for i in range(n_channels):\n",
    "        X_i = X[:, i]\n",
    "        X_rest = np.delete(X, i, axis=1)\n",
    "\n",
    "        model = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000)\n",
    "        model.fit(X_rest, X_i)\n",
    "\n",
    "        h[i] = model.intercept_[0]\n",
    "        coefs = model.coef_[0]\n",
    "        idx = 0\n",
    "        for j in range(n_channels):\n",
    "            if j == i:\n",
    "                continue\n",
    "            J[i, j] = coefs[idx]\n",
    "            idx += 1\n",
    "\n",
    "    # Symmetrize J\n",
    "    J = 0.5 * (J + J.T)\n",
    "\n",
    "    return h, J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5218e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ising_batch(binary_data):\n",
    "    ising_models = {}\n",
    "    for subj_id in tqdm(binary_data):\n",
    "        ising_models[subj_id] = {}\n",
    "        for band, binary in binary_data[subj_id].items():\n",
    "            X = binary.T  # shape: (timepoints, channels)\n",
    "            h, J = fit_pairwise_ising(X)\n",
    "            ising_models[subj_id][band] = {\n",
    "                'h': h,\n",
    "                'J': J\n",
    "            }\n",
    "    return ising_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load binarized data\n",
    "with open(\"binarized/binary_rest.pkl\", \"rb\") as f:\n",
    "    binary_rest = pickle.load(f)\n",
    "\n",
    "with open(\"binarized/binary_task.pkl\", \"rb\") as f:\n",
    "    binary_task = pickle.load(f)\n",
    "\n",
    "# Run fitting\n",
    "ising_rest = fit_ising_batch(binary_rest)\n",
    "ising_task = fit_ising_batch(binary_task)\n",
    "\n",
    "# Save results\n",
    "Path(\"ising_models\").mkdir(exist_ok=True)\n",
    "with open(\"ising_models/ising_rest.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ising_rest, f)\n",
    "\n",
    "with open(\"ising_models/ising_task.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ising_task, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def all_binary_states(n):\n",
    "    \"\"\"Return all 2^n binary states of length n as np.array of shape (2^n, n)\"\"\"\n",
    "    return np.array(list(product([1, -1], repeat=n)))  # use {-1, +1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738f43d1",
   "metadata": {},
   "source": [
    "Compute Energy\n",
    "E(s)=‚àíh^‚ä§*s‚àís^‚ä§*J*s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085f9835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_energies(states, h, J):\n",
    "    linear = - np.dot(states, h)\n",
    "    quad = - np.einsum('ij,ij->i', np.dot(states, J), states)\n",
    "    return linear + quad  # shape: (2^n,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb39c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_neighbors(index, n_bits):\n",
    "    \"\"\"\n",
    "    Return indices of neighbors of a binary state by flipping one bit.\n",
    "    Input: index ‚àà [0, 2^n), n_bits = number of bits (channels)\n",
    "    Output: list of neighbor indices\n",
    "    \"\"\"\n",
    "    return [index ^ (1 << i) for i in range(n_bits)]\n",
    "\n",
    "def find_local_minima(energies):\n",
    "    n_states = len(energies)\n",
    "    n_bits = int(np.log2(n_states))\n",
    "    minima = []\n",
    "    for i in range(n_states):\n",
    "        neighbors = hamming_neighbors(i, n_bits)\n",
    "        if all(energies[i] < energies[j] for j in neighbors):\n",
    "            minima.append(i)\n",
    "    return minima\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2edb5d",
   "metadata": {},
   "source": [
    "Combined Function to Run Per Subject/Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa4709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_landscape_features(h, J):\n",
    "    \"\"\"\n",
    "    Computes energy landscape features given Ising model parameters h and J.\n",
    "    Includes:\n",
    "        - num_minima\n",
    "        - minima_indices\n",
    "        - min_energy\n",
    "        - avg_energy\n",
    "        - energy_range\n",
    "        - energy_entropy (Boltzmann from normalized energies)\n",
    "    \"\"\"\n",
    "    n = len(h)\n",
    "    states = all_binary_states(n)  # shape: (2^n, n)\n",
    "    energies = compute_energies(states, h, J)  # shape: (2^n,)\n",
    "    \n",
    "    minima_indices = find_local_minima(energies)\n",
    "    num_minima = len(minima_indices)\n",
    "\n",
    "    # Normalize energies\n",
    "    norm_energies = (energies - energies.min()) / (energies.max() - energies.min())\n",
    "    probs = np.exp(-norm_energies)\n",
    "    probs /= probs.sum()\n",
    "\n",
    "    return {\n",
    "        'num_minima': num_minima,\n",
    "        'minima_indices': minima_indices,\n",
    "        'min_energy': float(np.min(energies)),\n",
    "        'avg_energy': float(np.mean(energies)),\n",
    "        'energy_range': float(np.max(energies) - np.min(energies)),\n",
    "        'energy_entropy': float(entropy(probs))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8383ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_extract_features(ising_dict):\n",
    "    all_features = {}\n",
    "    for subj_id in tqdm(ising_dict):\n",
    "        all_features[subj_id] = {}\n",
    "        for band in ising_dict[subj_id]:\n",
    "            h = ising_dict[subj_id][band]['h']\n",
    "            J = ising_dict[subj_id][band]['J']\n",
    "            all_features[subj_id][band] = extract_landscape_features(h, J)\n",
    "    return all_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3547805",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ising_models/ising_rest.pkl\", \"rb\") as f:\n",
    "    ising_rest = pickle.load(f)\n",
    "\n",
    "with open(\"ising_models/ising_task.pkl\", \"rb\") as f:\n",
    "    ising_task = pickle.load(f)\n",
    "\n",
    "# Run feature extraction\n",
    "features_rest = batch_extract_features(ising_rest)\n",
    "features_task = batch_extract_features(ising_task)\n",
    "\n",
    "# Save\n",
    "Path(\"landscape_features\").mkdir(exist_ok=True)\n",
    "with open(\"landscape_features/features_rest.pkl\", \"wb\") as f:\n",
    "    pickle.dump(features_rest, f)\n",
    "\n",
    "with open(\"landscape_features/features_task.pkl\", \"wb\") as f:\n",
    "    pickle.dump(features_task, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac4522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of subjects\n",
    "print(\"‚úÖ Number of subjects:\", len(features_task))\n",
    "\n",
    "# Check number of features for first subject\n",
    "first_subj = list(features_task.keys())[0]\n",
    "first_features = features_task[first_subj]\n",
    "\n",
    "# If it's nested (e.g., by band), flatten and count\n",
    "flat_features = {}\n",
    "\n",
    "for band, band_feats in first_features.items():\n",
    "    for feat_name in band_feats:\n",
    "        flat_features[f\"{band}_{feat_name}\"] = band_feats[feat_name]\n",
    "\n",
    "print(\"‚úÖ Total number of features for one subject:\", len(flat_features))\n",
    "print(\"üìã Sample feature names:\", list(flat_features.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good/Bad group definitions using numeric keys as strings\n",
    "bad_counters = ['0', '4', '6', '9', '10', '14', '19', '21', '22', '30']\n",
    "good_counters = ['1', '2', '3', '5', '7', '8', '11', '12', '13', '15', '16', '17',\n",
    "                 '18', '20', '23', '24', '25', '26', '27', '28', '29', '31', '32',\n",
    "                 '33', '34', '35']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_group_features(features_dict, subjects, band='alpha'):\n",
    "    summary_list = []\n",
    "    for sid in subjects:\n",
    "        if sid in features_dict and band in features_dict[sid]:\n",
    "            summary = summarize_features(features_dict[sid][band])\n",
    "            summary_list.append(summary)\n",
    "    return summary_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411df07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "\n",
    "def compare_groups(good, bad, feature_name):\n",
    "    good_vals = [s[feature_name] for s in good]\n",
    "    bad_vals = [s[feature_name] for s in bad]\n",
    "\n",
    "    # Non-parametric test\n",
    "    stat, pval = mannwhitneyu(good_vals, bad_vals, alternative='two-sided')\n",
    "\n",
    "    print(f\"\\nüìä Feature: {feature_name}\")\n",
    "    print(f\"  Mean (Good): {np.mean(good_vals):.3f}\")\n",
    "    print(f\"  Mean (Bad) : {np.mean(bad_vals):.3f}\")\n",
    "    print(f\"  Mann-Whitney U test: p = {pval:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features\n",
    "with open(\"landscape_features/features_rest.pkl\", \"rb\") as f:\n",
    "    features_rest = pickle.load(f)\n",
    "\n",
    "# Choose band\n",
    "band = 'theta'\n",
    "\n",
    "# Group-wise summaries\n",
    "summary_good = aggregate_group_features(features_rest, good_counters, band=band)\n",
    "summary_bad = aggregate_group_features(features_rest, bad_counters, band=band)\n",
    "\n",
    "# Compare features\n",
    "compare_groups(summary_good, summary_bad, 'num_minima')\n",
    "compare_groups(summary_good, summary_bad, 'energy_entropy')\n",
    "compare_groups(summary_good, summary_bad, 'min_energy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d0dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3296ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distribution(good, bad, feature_name, band):\n",
    "    df = pd.DataFrame({\n",
    "        'Value': [s[feature_name] for s in good] + [s[feature_name] for s in bad],\n",
    "        'Group': ['Good'] * len(good) + ['Bad'] * len(bad),\n",
    "        'Feature': feature_name\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x='Group', y='Value', data=df, palette='Set2')\n",
    "    plt.title(f\"{feature_name} - {band} band\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2fe8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_distribution(summary_good, summary_bad, 'energy_entropy', band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee96bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_energy_histogram(features_dict, subject_id, band='alpha', normalized=False):\n",
    "    energies = features_dict[subject_id][band]['normalized_energies'] if normalized \\\n",
    "               else features_dict[subject_id][band]['energies']\n",
    "    \n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.hist(energies, bins=100, color='slateblue', alpha=0.7)\n",
    "    plt.title(f\"{'Normalized' if normalized else 'Raw'} Energy Distribution\\nSubject {subject_id} | Band: {band}\")\n",
    "    plt.xlabel(\"Energy\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15333bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_energy_histogram(features_rest, subject_id='3', band='alpha', normalized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec332ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_minima_energies(features_dict, subject_id, band='alpha'):\n",
    "    all_energies = features_dict[subject_id][band]['energies']\n",
    "    minima_indices = features_dict[subject_id][band]['minima_indices']\n",
    "    minima_energies = all_energies[minima_indices]\n",
    "\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.hist(minima_energies, bins=20, color='darkorange', alpha=0.8)\n",
    "    plt.title(f\"Minima Energies\\nSubject {subject_id} | Band: {band}\")\n",
    "    plt.xlabel(\"Energy\")\n",
    "    plt.ylabel(\"Number of Minima\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2580c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_minima_energies(features_rest, subject_id='3', band='alpha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_summary(features_dict, subject_id, band='alpha'):\n",
    "    f = features_dict[subject_id][band]\n",
    "    print(f\"\\nüß† Subject {subject_id} | Band: {band}\")\n",
    "    print(f\"  ‚ñ∏ Num minima        : {f['num_minima']}\")\n",
    "    print(f\"  ‚ñ∏ Min energy        : {np.min(f['energies']):.3f}\")\n",
    "    print(f\"  ‚ñ∏ Energy entropy    : {entropy(np.exp(-f['normalized_energies']) / np.sum(np.exp(-f['normalized_energies']))):.3f}\")\n",
    "    print(f\"  ‚ñ∏ Avg energy        : {np.mean(f['energies']):.3f}\")\n",
    "    print(f\"  ‚ñ∏ Energy range      : {np.max(f['energies']) - np.min(f['energies']):.3f}\")\n",
    "    print(f\"  ‚ñ∏ # State samples   : {len(f['energies'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5fe1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_feature_summary(features_rest, subject_id='3', band='gamma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba1cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_features(features_dict, subject_ids, band):\n",
    "    keys = ['num_minima', 'min_energy', 'energy_entropy', 'avg_energy', 'energy_range']\n",
    "    all_features = {k: [] for k in keys}\n",
    "    all_features['minima_indices_len'] = []\n",
    "\n",
    "    for sid in subject_ids:\n",
    "        try:\n",
    "            band_feats = features_dict[sid].get(band, {})\n",
    "            for k in keys:\n",
    "                all_features[k].append(band_feats[k])\n",
    "\n",
    "            # Convert minima_indices to a scalar (length)\n",
    "            all_features['minima_indices_len'].append(len(band_feats['minima_indices']))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Skipping subject {sid} for band {band}: {e}\")\n",
    "\n",
    "    return all_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b6a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "def compare_feature(feature, good_vals, bad_vals, band):\n",
    "    g_mean = np.mean(good_vals)\n",
    "    b_mean = np.mean(bad_vals)\n",
    "    stat, pval = mannwhitneyu(good_vals, bad_vals, alternative='two-sided')\n",
    "    \n",
    "    print(f\"üìä {feature.upper()} ({band}):\")\n",
    "    print(f\"   Good mean: {g_mean:.3f} | Bad mean: {b_mean:.3f} | p = {pval:.4f}\")\n",
    "    return {'feature': feature, 'band': band, 'good_mean': g_mean, 'bad_mean': b_mean, 'p': pval}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a7455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_group_comparison(features_dict, good_ids, bad_ids, bands=['theta', 'alpha', 'beta', 'gamma', 'broadband']):\n",
    "    results = []\n",
    "    for band in bands:\n",
    "        good_feats = get_group_features(features_dict, good_ids, band)\n",
    "        bad_feats = get_group_features(features_dict, bad_ids, band)\n",
    "\n",
    "        for feature in good_feats:\n",
    "            result = compare_feature(feature, good_feats[feature], bad_feats[feature], band)\n",
    "            results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c703180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"landscape_features/features_task.pkl\", \"rb\") as f:\n",
    "    features_task = pickle.load(f)\n",
    "\n",
    "# Run\n",
    "results_task = full_group_comparison(features_task, good_counters, bad_counters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20682896",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"landscape_features/features_rest.pkl\", \"rb\") as f:\n",
    "    features_rest = pickle.load(f)\n",
    "\n",
    "# Run\n",
    "results_rest = full_group_comparison(features_rest, good_counters, bad_counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec168a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"all_features\").mkdir(exist_ok=True)\n",
    "\n",
    "# Save the dictionary for ML use\n",
    "with open(\"all_features/Advance_features_rest.pkl\", \"wb\") as f:\n",
    "    pickle.dump(features_rest, f)\n",
    "\n",
    "print(\"‚úÖ Saved Advance features to all_features/Advance_features_rest.pkl\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b6ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary for ML use\n",
    "with open(\"all_features/Advance_features_task.pkl\", \"wb\") as f:\n",
    "    pickle.dump(features_task, f)\n",
    "\n",
    "print(\"‚úÖ Saved Advance features to all_features/Advance_features_task.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d8ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"landscape_features/features_task.pkl\", \"rb\") as f:\n",
    "    features_rest = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "def bayesian_compare(good_vals, bad_vals, feature, band):\n",
    "    with pm.Model() as model:\n",
    "        mu_good = pm.Normal(\"mu_good\", mu=0, sigma=10)\n",
    "        mu_bad = pm.Normal(\"mu_bad\", mu=0, sigma=10)\n",
    "        sigma = pm.HalfNormal(\"sigma\", sigma=10)\n",
    "        \n",
    "        y_good = pm.Normal(\"y_good\", mu=mu_good, sigma=sigma, observed=good_vals)\n",
    "        y_bad = pm.Normal(\"y_bad\", mu=mu_bad, sigma=sigma, observed=bad_vals)\n",
    "        \n",
    "        diff = pm.Deterministic(\"diff\", mu_good - mu_bad)\n",
    "        trace = pm.sample(2000, tune=1000, target_accept=0.95, progressbar=False, return_inferencedata=True)\n",
    "        '''trace = pm.sample(\n",
    "        draws=2000,\n",
    "        tune=1000,\n",
    "        target_accept=0.95,        # ‚Üê More conservative, fewer divergences\n",
    "        max_treedepth=15,          # ‚Üê Allows deeper exploration of the posterior\n",
    "        return_inferencedata=True,\n",
    "        progressbar=False\n",
    "        )'''\n",
    "\n",
    "    az.plot_posterior(trace, var_names=[\"diff\"], ref_val=0)\n",
    "    summary = az.summary(trace, var_names=[\"diff\"], hdi_prob=0.95)\n",
    "\n",
    "    print(f\"\\nüìä Bayesian comparison for {feature.upper()} in {band.upper()}:\")\n",
    "    print(summary)\n",
    "    \n",
    "    return {\n",
    "        'feature': feature,\n",
    "        'band': band,\n",
    "        'diff_mean': summary.loc['diff', 'mean'],\n",
    "        'hdi_2.5%': summary.loc['diff', 'hdi_2.5%'],\n",
    "        'hdi_97.5%': summary.loc['diff', 'hdi_97.5%'],\n",
    "        'prob_positive': (trace.posterior[\"diff\"] > 0).mean().item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_group_features(features_dict, subjects, band):\n",
    "    feature_list = []\n",
    "    for sid in subjects:\n",
    "        try:\n",
    "            if band in features_dict[sid]:\n",
    "                feature_list.append(extract_all_features(features_dict[sid][band]))\n",
    "        except:\n",
    "            continue\n",
    "    return feature_list\n",
    "\n",
    "def run_bayesian_analysis_all(features_dict, good_ids, bad_ids, bands=['theta', 'alpha', 'beta', 'gamma', 'broadband']):\n",
    "    results = []\n",
    "    for band in bands:\n",
    "        good = aggregate_group_features(features_dict, good_ids, band)\n",
    "        bad = aggregate_group_features(features_dict, bad_ids, band)\n",
    "\n",
    "        if not good or not bad:\n",
    "            continue\n",
    "\n",
    "        for feature in good[0].keys():\n",
    "            try:\n",
    "                good_vals = [g[feature] for g in good]\n",
    "                bad_vals = [b[feature] for b in bad]\n",
    "                result = bayesian_compare(good_vals, bad_vals, feature, band)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Skipped {feature} in {band}: {e}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bayesian_analysis_all(features_dict, good_ids=good_counters, bad_ids=bad_counters, bands=['theta', 'alpha', 'beta', 'gamma']):\n",
    "    results = []\n",
    "    for band in bands:\n",
    "        good_feats = get_group_features(features_dict, good_ids, band)\n",
    "        bad_feats = get_group_features(features_dict, bad_ids, band)\n",
    "\n",
    "        for feature in good_feats:\n",
    "            try:\n",
    "                good_vals = good_feats[feature]\n",
    "                bad_vals = bad_feats[feature]\n",
    "                result = bayesian_compare(good_vals, bad_vals, feature, band)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Skipped {feature} in {band}: {e}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d719ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_bayesian_analysis_all(features_rest, good_counters, bad_counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6240f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_bayesian_analysis_all(features_task, good_counters, bad_counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fa253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input data\n",
    "data = pd.DataFrame({\n",
    "    'Band': ['theta', 'alpha', 'beta', 'gamma', 'broadband'] * 2,\n",
    "    'Condition': ['Task'] * 5 + ['Rest'] * 5,\n",
    "    'Good': [36.462, 33.231, 31.077, 30.385, 34.077,\n",
    "             32.346, 30.962, 31.769, 31.923, 30.346],\n",
    "    'Bad':  [42.500, 35.900, 35.200, 35.800, 32.300,\n",
    "             35.300, 29.500, 28.900, 34.700, 25.500],\n",
    "    'p_val': [0.0890, 0.6453, 0.7906, 0.1672, 0.7105,\n",
    "              0.2088, 1.0000, 0.5711, 0.3462, 0.1839]\n",
    "})\n",
    "\n",
    "# Melt for violin plot\n",
    "df_long = data.melt(id_vars=['Band', 'Condition', 'p_val'], \n",
    "                    value_vars=['Good', 'Bad'], \n",
    "                    var_name='Group', value_name='Num Minima')\n",
    "\n",
    "# Plot violin\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.violinplot(data=df_long, x='Band', y='Num Minima', hue='Group',\n",
    "               split=True, inner='box', palette='pastel', bw_adjust=0.2)\n",
    "\n",
    "# Overlay red dots for Task, green for Rest\n",
    "for i, band in enumerate(['theta', 'alpha', 'beta', 'gamma', 'broadband']):\n",
    "    # Get task and rest values\n",
    "    task_vals = data[(data['Band'] == band) & (data['Condition'] == 'Task')][['Good', 'Bad']].values.flatten()\n",
    "    rest_vals = data[(data['Band'] == band) & (data['Condition'] == 'Rest')][['Good', 'Bad']].values.flatten()\n",
    "    \n",
    "    # Plot task dots in red\n",
    "    plt.scatter([i - 0.15, i + 0.15], task_vals, color='red', label='Task' if i == 0 else \"\", zorder=5)\n",
    "    \n",
    "    # Plot rest dots in green\n",
    "    plt.scatter([i - 0.15, i + 0.15], rest_vals, color='green', label='Rest' if i == 0 else \"\", zorder=5)\n",
    "\n",
    "# Add horizontal p-value text\n",
    "y_label_pos = 22\n",
    "for i, band in enumerate(['theta', 'alpha', 'beta', 'gamma', 'broadband']):\n",
    "    task_p = data[(data['Band'] == band) & (data['Condition'] == 'Task')]['p_val'].values[0]\n",
    "    rest_p = data[(data['Band'] == band) & (data['Condition'] == 'Rest')]['p_val'].values[0]\n",
    "    plt.text(i, y_label_pos, f\"Task p = {task_p:.3f}\", ha='center', fontsize=9, color='red', weight='bold')\n",
    "    plt.text(i, y_label_pos - 1.5, f\"Rest p = {rest_p:.3f}\", ha='center', fontsize=9, color='green', weight='bold')\n",
    "\n",
    "# Customize\n",
    "#plt.title(\"Number of Minima across Frequency Bands (Task vs Rest)\", fontsize=14)\n",
    "plt.xlabel(\"Frequency Band\")\n",
    "plt.ylabel(\"Number of Attractor Minima\")\n",
    "plt.ylim(20, 45)\n",
    "plt.legend(title=\"Group\", loc=\"upper right\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "'''# Explanation box\n",
    "plt.text(-0.25, 17.5, \"Red = Task minima\\nGreen = Rest minima\",\n",
    "         fontsize=10, bbox=dict(facecolor='white', edgecolor='gray', boxstyle='round,pad=0.5'))'''\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c810bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace with your real PyMC3 posterior diff samples\n",
    "posterior = {\n",
    "    \"Theta (Task)\": np.random.normal(loc=-3.47, scale=3.66, size=4000),\n",
    "    \"Theta (Rest)\": np.random.normal(loc=-1.16, scale=3.43, size=4000)\n",
    "}\n",
    "\n",
    "# Convert to InferenceData\n",
    "idata = az.from_dict(posterior=posterior)\n",
    "\n",
    "# Plot with HDI\n",
    "az.plot_posterior(idata,\n",
    "                  kind='hist',\n",
    "                  hdi_prob=0.95,\n",
    "                  rope=(-1, 1),  # e.g., define ROPE if meaningful\n",
    "                  ref_val=0,\n",
    "                  figsize=(8, 4),\n",
    "                  textsize=12,\n",
    "                  point_estimate='mean')\n",
    "\n",
    "plt.suptitle(\"Bayesian Posterior for ŒîNUM_MINIMA (Good - Bad)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f8e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate posterior samples\n",
    "posterior_task = np.random.normal(loc=-3.465, scale=3.658, size=4000)\n",
    "posterior_rest = np.random.normal(loc=-1.16, scale=3.427, size=4000)\n",
    "\n",
    "idata = az.from_dict(posterior={\n",
    "    \"Theta_NUM_MINIMA_Task\": posterior_task,\n",
    "    \"Theta_NUM_MINIMA_Rest\": posterior_rest\n",
    "})\n",
    "\n",
    "# Plot with HDI and ROPE\n",
    "az.plot_posterior(idata,\n",
    "                  hdi_prob=0.95,\n",
    "                  rope=(-1, 1),\n",
    "                  ref_val=0,\n",
    "                  figsize=(10, 4),\n",
    "                  textsize=12)\n",
    "\n",
    "plt.suptitle(\"üìä Bayesian Posterior: Num Minima Difference (Good - Bad)\", fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba35b5e6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
