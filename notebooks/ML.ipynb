{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9299b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from pathlib import Path    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc69c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load Hopfield features\n",
    "with open(\"all_features/hopf_features_task.pkl\", \"rb\") as f:\n",
    "    hopf_rest = pickle.load(f)\n",
    "\n",
    "# Load Ising/Jansen-Rit features\n",
    "with open(\"all_features/rit_features_task.pkl\", \"rb\") as f:\n",
    "    rit_rest = pickle.load(f)\n",
    "\n",
    "# Load Advanced Ising landscape features\n",
    "with open(\"all_features/Advance_features_task.pkl\", \"rb\") as f:\n",
    "    adv_rest = pickle.load(f)\n",
    "\n",
    "# Print keys and a sample\n",
    "print(\"🔹 Hopfield Feature Sample:\")\n",
    "first_key = next(iter(hopf_rest))\n",
    "print(f\"Subject ID: {first_key}\")\n",
    "print(hopf_rest[first_key])  # dict of band → feature dict\n",
    "\n",
    "print(\"\\n🔹 Jansen-Rit Feature Sample:\")\n",
    "first_key = next(iter(rit_rest))\n",
    "print(f\"Subject ID: {first_key}\")\n",
    "print(rit_rest[first_key])  # dict of model params\n",
    "\n",
    "print(\"\\n🔹 Advanced Energy Landscape Feature Sample:\")\n",
    "first_key = next(iter(adv_rest))\n",
    "print(f\"Subject ID: {first_key}\")\n",
    "print(adv_rest[first_key])  # dict of band → feature dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f137c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def flatten_features(subject_id, hopf_feats, rit_feats, adv_feats):\n",
    "    flat = {\"subject\": subject_id}\n",
    "\n",
    "    # Hopfield features\n",
    "    if subject_id in hopf_feats:\n",
    "        for band, band_feats in hopf_feats[subject_id].items():\n",
    "            for k, v in band_feats.items():\n",
    "                flat[f\"{band}_{k}\"] = float(v)\n",
    "\n",
    "    # Jansen-Rit features\n",
    "    if subject_id in rit_feats:\n",
    "        for k, v in rit_feats[subject_id].items():\n",
    "            flat[f\"rit_{k}\"] = float(v)\n",
    "\n",
    "    # Advanced features\n",
    "    if subject_id in adv_feats:\n",
    "        for band, band_feats in adv_feats[subject_id].items():\n",
    "            if \"num_minima\" in band_feats:\n",
    "                flat[f\"{band}_num_minima\"] = band_feats[\"num_minima\"]\n",
    "            # Optionally: add summary statistics of energies\n",
    "            energies = band_feats.get(\"normalized_energies\")\n",
    "            if energies is not None:\n",
    "                flat[f\"{band}_energy_mean\"] = float(np.mean(energies))\n",
    "                flat[f\"{band}_energy_std\"] = float(np.std(energies))\n",
    "\n",
    "    return flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Flatten all subjects into a list of feature dictionaries\n",
    "all_subjects = list(set(hopf_rest) | set(rit_rest) | set(adv_rest))\n",
    "\n",
    "flattened_list = []\n",
    "for subj_id in all_subjects:\n",
    "    features = flatten_features(subj_id, hopf_rest, rit_rest, adv_rest)\n",
    "    flattened_list.append(features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(flattened_list)\n",
    "\n",
    "print(f\"✅ Combined feature DataFrame created with shape: {df.shape}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define good and bad subject IDs\n",
    "good_ids = ['1', '2', '3', '5', '7', '8', \n",
    "            '11', '12', '13', '15', '16', '17', \n",
    "            '18', '20', '23', '24', '25', '26', \n",
    "            '27', '28', '29', '31', '32', '33', \n",
    "            '34', '35']\n",
    "\n",
    "bad_ids = ['0', '4', '6', '9', '10', \n",
    "           '14', '19', '21', '22', '30']\n",
    "\n",
    "# Add labels to DataFrame\n",
    "def get_label(subj_id):\n",
    "    if subj_id in good_ids:\n",
    "        return 1\n",
    "    elif subj_id in bad_ids:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan  # drop if unknown\n",
    "\n",
    "df[\"label\"] = df[\"subject\"].astype(str).apply(get_label)\n",
    "df = df.dropna(subset=[\"label\"])  # drop rows without label\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "print(f\"✅ Labels added. Final shape: {df.shape}\")\n",
    "print(\"Class distribution:\\n\", df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6aba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_features/combined_features_rest.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "print(\"✅ Saved combined DataFrame to all_features/combined_features_rest.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1a22b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Load All Feature Files ===\n",
    "def load_features():\n",
    "    with open(\"all_features/hopf_features_rest.pkl\", \"rb\") as f: hopf_rest = pickle.load(f)\n",
    "    with open(\"all_features/rit_features_rest.pkl\", \"rb\") as f: rit_rest = pickle.load(f)\n",
    "    with open(\"all_features/Advance_features_rest.pkl\", \"rb\") as f: adv_rest = pickle.load(f)\n",
    "\n",
    "    with open(\"all_features/hopf_features_task.pkl\", \"rb\") as f: hopf_task = pickle.load(f)\n",
    "    with open(\"all_features/rit_features_task.pkl\", \"rb\") as f: rit_task = pickle.load(f)\n",
    "    with open(\"all_features/Advance_features_task.pkl\", \"rb\") as f: adv_task = pickle.load(f)\n",
    "\n",
    "    return hopf_rest, rit_rest, adv_rest, hopf_task, rit_task, adv_task\n",
    "\n",
    "# === Define flattening function ===\n",
    "def flatten_features(subject_id, hopf, rit, adv, prefix=\"rest\"):\n",
    "    flat = {\"subject\": subject_id}\n",
    "\n",
    "    # Hopfield\n",
    "    if subject_id in hopf:\n",
    "        for band, band_feats in hopf[subject_id].items():\n",
    "            for k, v in band_feats.items():\n",
    "                flat[f\"{prefix}_{band}_{k}\"] = float(v)\n",
    "\n",
    "    # Jansen-Rit\n",
    "    if subject_id in rit:\n",
    "        for k, v in rit[subject_id].items():\n",
    "            flat[f\"{prefix}_rit_{k}\"] = float(v)\n",
    "\n",
    "    # Advanced landscape\n",
    "    if subject_id in adv:\n",
    "        for band, band_feats in adv[subject_id].items():\n",
    "            if \"num_minima\" in band_feats:\n",
    "                flat[f\"{prefix}_{band}_num_minima\"] = band_feats[\"num_minima\"]\n",
    "            energies = band_feats.get(\"normalized_energies\")\n",
    "            if energies is not None:\n",
    "                flat[f\"{prefix}_{band}_energy_mean\"] = float(np.mean(energies))\n",
    "                flat[f\"{prefix}_{band}_energy_std\"] = float(np.std(energies))\n",
    "\n",
    "    return flat\n",
    "\n",
    "# === Get Group Labels ===\n",
    "good_ids = ['1','2','3','5','7','8','11','12','13','15','16','17','18','20','23','24','25','26','27','28','29','31','32','33','34','35']\n",
    "bad_ids = ['0','4','6','9','10','14','19','21','22','30']\n",
    "\n",
    "# === Run ===\n",
    "hopf_rest, rit_rest, adv_rest, hopf_task, rit_task, adv_task = load_features()\n",
    "\n",
    "# Combine all subject IDs\n",
    "all_subjects = list(set(hopf_rest) | set(rit_rest) | set(adv_rest) | set(hopf_task) | set(rit_task) | set(adv_task))\n",
    "\n",
    "combined = []\n",
    "for sid in all_subjects:\n",
    "    row = {}\n",
    "    row.update(flatten_features(sid, hopf_rest, rit_rest, adv_rest, prefix=\"rest\"))\n",
    "    row.update(flatten_features(sid, hopf_task, rit_task, adv_task, prefix=\"task\"))\n",
    "    row[\"subject\"] = sid\n",
    "    row[\"label\"] = 1 if sid in good_ids else 0\n",
    "    combined.append(row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(combined)\n",
    "print(f\"✅ Combined REST+TASK feature DataFrame created: {df.shape}\")\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "# Save for ML use\n",
    "df.to_pickle(\"all_features/combined_features_rest_task.pkl.pkl\")\n",
    "print(\"💾 Saved as all_features/combined_features_rest_task.pkl.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b06567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Load combined features ===\n",
    "with open(\"all_features/combined_features_rest.pkl\", \"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "# === Clean any dict/list type values\n",
    "df = df.applymap(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "\n",
    "# === Split features and labels\n",
    "X = df.drop(columns=[\"subject\", \"label\"])\n",
    "y = df[\"label\"]\n",
    "\n",
    "# === Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# === Gradient Boosting Classifier (XGBoost)\n",
    "model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# === Cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_pred = cross_val_predict(model, X_scaled, y, cv=cv)\n",
    "\n",
    "# === Evaluation\n",
    "acc = accuracy_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred, average='macro')\n",
    "\n",
    "print(f\"\\n✅ Accuracy: {acc:.3f}\")\n",
    "print(f\"✅ F1 Macro Score: {f1:.3f}\\n\")\n",
    "\n",
    "print(\"📊 Classification Report:\\n\", classification_report(y, y_pred))\n",
    "\n",
    "# === Confusion Matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Bad\", \"Good\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"XGBoost - Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f03c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "model.fit(X_scaled, y)\n",
    "importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Sort by importance\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=importances[sorted_idx][:15], y=feature_names[sorted_idx][:15])\n",
    "plt.title(\"Top 15 Feature Importances (XGBoost)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(feature_names[sorted_idx][:15],importances[sorted_idx][:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['alpha_avg_energy', 'alpha_avg_steps', 'gamma_avg_energy',\n",
    "                'alpha_basin_entropy', 'alpha_energy_mean', 'gamma_avg_steps',\n",
    "                'alpha_num_minima', 'theta_n_attractors', 'gamma_energy_std', 'rit_C']\n",
    "\n",
    "X_top = df[top_features]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_top, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# Retrain model\n",
    "from xgboost import XGBClassifier\n",
    "xgb_top = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_top.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "y_pred = xgb_top.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if column error\n",
    "\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Features you want\n",
    "top_features = ['alpha_avg_energy', 'alpha_avg_steps', 'gamma_avg_energy',\n",
    "                'alpha_basin_entropy', 'alpha_energy_mean', 'gamma_avg_steps',\n",
    "                'alpha_num_minima', 'theta_n_attractors', 'gamma_energy_std', 'rit_C']\n",
    "\n",
    "# All available features\n",
    "available_features = df.columns.tolist()\n",
    "\n",
    "# Find best matches for each\n",
    "for feat in top_features:\n",
    "    match = get_close_matches(feat, available_features, n=1)\n",
    "    print(f\"{feat} → {match[0] if match else '❌ No match found'}\")\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# === Define top desired features ===\n",
    "desired_features = [\n",
    "    'alpha_avg_energy', 'alpha_avg_steps', 'gamma_avg_energy',\n",
    "    'alpha_basin_entropy', 'alpha_energy_mean', 'gamma_avg_steps',\n",
    "    'alpha_num_minima', 'theta_n_attractors', 'gamma_energy_std', 'rit_C'\n",
    "]\n",
    "\n",
    "# === Fix column name mismatches ===\n",
    "actual_cols = df.columns.tolist()\n",
    "selected_features = []\n",
    "\n",
    "print(\"🔍 Matching selected features to available columns...\\n\")\n",
    "for feat in desired_features:\n",
    "    match = get_close_matches(feat, actual_cols, n=1)\n",
    "    if match:\n",
    "        selected_features.append(match[0])\n",
    "        print(f\"✅ {feat:25} → {match[0]}\")\n",
    "    else:\n",
    "        print(f\"❌ {feat:25} → No match found\")\n",
    "\n",
    "# === Prepare features and label ===\n",
    "X = df[selected_features]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# === Optional: Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# === Apply SMOTE for class balance\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_scaled, y)\n",
    "\n",
    "# === Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled)\n",
    "\n",
    "# === Train XGBoost model\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"\\n📊 Classification Report After SMOTE:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"✅ Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "# === Feature importance plot\n",
    "importances = model.feature_importances_\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(selected_features, importances)\n",
    "plt.title(\"🔬 XGBoost Feature Importance\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e61fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# === Select top features from before\n",
    "top_features = ['alpha_avg_energy', 'alpha_avg_steps', 'gamma_avg_energy',\n",
    "                'alpha_basin_entropy', 'alpha_energy_mean', 'gamma_avg_steps',\n",
    "                'alpha_num_minima', 'theta_n_attractors', 'gamma_energy_std', 'rit_C']\n",
    "\n",
    "X = df[top_features]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# === Apply SMOTE to balance data\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
    "\n",
    "# === Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled)\n",
    "\n",
    "# === Train XGBoost\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\n📊 Classification Report After SMOTE:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06364680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Resample\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X, y)\n",
    "\n",
    "# Step 2: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled\n",
    ")\n",
    "\n",
    "# Step 3: Initialize model\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Step 4: Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 6: Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "labels = ['Bad', 'Good']\n",
    "conf_matrix_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[f'Actual {label}' for label in labels],\n",
    "    columns=[f'Predicted {label}' for label in labels]\n",
    ")\n",
    "\n",
    "print(\"\\n🧮 Confusion Matrix:\\n\")\n",
    "print(conf_matrix_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77641f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Load combined features ===\n",
    "with open(\"all_features/combined_features_rest.pkl\", \"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "# === Clean any dict/list type values\n",
    "df = df.applymap(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) else x)\n",
    "\n",
    "# === Split features and labels\n",
    "X = df.drop(columns=[\"subject\", \"label\"])\n",
    "y = df[\"label\"]\n",
    "\n",
    "# === Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# === Gradient Boosting Classifier (XGBoost)\n",
    "model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# === Cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_pred = cross_val_predict(model, X_scaled, y, cv=cv)\n",
    "\n",
    "# === Evaluation\n",
    "acc = accuracy_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred, average='macro')\n",
    "\n",
    "print(f\"\\n✅ Accuracy: {acc:.3f}\")\n",
    "print(f\"✅ F1 Macro Score: {f1:.3f}\\n\")\n",
    "\n",
    "print(\"📊 Classification Report:\\n\", classification_report(y, y_pred))\n",
    "\n",
    "# === Confusion Matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Bad\", \"Good\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"XGBoost - Confusion Matrix\")\n",
    "plt.show()\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay\n",
    "\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "RocCurveDisplay.from_predictions(y_test, y_prob)\n",
    "plt.title(\"XGBoost ROC Curve\")\n",
    "plt.show()\n",
    "print(f\"🧮 AUC Score: {roc_auc_score(y_test, y_prob):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Use your final dataframe\n",
    "features_to_plot = [\n",
    "    'alpha_avg_energy', 'alpha_avg_steps', 'gamma_avg_energy',\n",
    "    'alpha_basin_entropy', 'alpha_energy_mean', 'gamma_avg_steps',\n",
    "    'alpha_num_minima', 'theta_n_attractors', 'gamma_energy_std', 'rit_C'\n",
    "]\n",
    "\n",
    "for feat in features_to_plot:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x=\"label\", y=feat, data=df)\n",
    "    sns.stripplot(x=\"label\", y=feat, data=df, color='black', alpha=0.5)\n",
    "    plt.title(f\"Feature: {feat}\")\n",
    "    plt.xticks([0, 1], [\"Bad\", \"Good\"])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bde458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "for feat in features_to_plot:\n",
    "    group0 = df[df[\"label\"] == 0][feat]\n",
    "    group1 = df[df[\"label\"] == 1][feat]\n",
    "    stat, p = mannwhitneyu(group0, group1)\n",
    "    print(f\"{feat:25s} | p = {p:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
