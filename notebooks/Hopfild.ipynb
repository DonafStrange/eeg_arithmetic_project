{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58904bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import mne\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import entropy\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "from mne.preprocessing import ICA\n",
    "from scipy.signal import butter, filtfilt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def all_binary_states(n):\n",
    "    \"\"\"\n",
    "    Generate all 2^n binary states in {-1, +1}^n\n",
    "    Output shape: (2^n, n)\n",
    "    \"\"\"\n",
    "    return np.array(list(product([-1, 1], repeat=n)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d57cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"binarized/binary_task.pkl\", \"rb\") as f:\n",
    "    binary_task = pickle.load(f)\n",
    "\n",
    "with open(\"binarized/binary_rest.pkl\", \"rb\") as f:\n",
    "    binary_rest = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b329142",
   "metadata": {},
   "source": [
    "with open(\"ising_models/ising_task.pkl\", \"rb\") as f:\n",
    "    ising_task = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_patterns(binary_data, bands=['theta', 'alpha', 'beta', 'gamma', 'broadband'], max_patterns=100):\n",
    "    \"\"\"\n",
    "    Extract binary EEG state patterns for all subjects and bands, ensure {-1, +1}.\n",
    "    Returns:\n",
    "        patterns_dict[subject][band] = (n_patterns, n_channels)\n",
    "    \"\"\"\n",
    "    patterns_dict = {}\n",
    "\n",
    "    for subj_id in binary_data:\n",
    "        patterns_dict[subj_id] = {}\n",
    "        for band in bands:\n",
    "            try:\n",
    "                X = binary_data[subj_id][band].T  # (timepoints, channels)\n",
    "\n",
    "                # Sample max_patterns timepoints\n",
    "                if X.shape[0] > max_patterns:\n",
    "                    idx = np.random.choice(X.shape[0], size=max_patterns, replace=False)\n",
    "                    patterns = X[idx]\n",
    "                else:\n",
    "                    patterns = X\n",
    "\n",
    "                # ‚úÖ Enforce binary ‚Üí bipolar conversion\n",
    "                patterns = np.where(patterns == 0, -1, 1)\n",
    "\n",
    "                # Sanity check\n",
    "                if not np.all(np.isin(patterns, [-1, 1])):\n",
    "                    raise ValueError(\"Pattern conversion failed!\")\n",
    "\n",
    "                patterns_dict[subj_id][band] = patterns\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Skipping subject {subj_id}, band {band}: {e}\")\n",
    "    \n",
    "    return patterns_dict\n",
    "\n",
    "patterns_rest = extract_all_patterns(binary_rest, bands=['theta', 'alpha', 'beta', 'gamma', 'broadband'], max_patterns=100)\n",
    "patterns_task = extract_all_patterns(binary_task, bands=['theta', 'alpha', 'beta', 'gamma', 'broadband'], max_patterns=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa07e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(patterns_rest['22']['beta'].shape)  # e.g., (100, 19)\n",
    "print(patterns_task['22']['beta'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b32534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HopfieldNetwork:\n",
    "    def __init__(self, n_units):\n",
    "        self.n_units = n_units\n",
    "        self.J = np.zeros((n_units, n_units))  # connectivity matrix\n",
    "\n",
    "    def train_hebbian(self, patterns):\n",
    "        \"\"\"\n",
    "        Train the network using Hebbian learning.\n",
    "        patterns: (n_patterns, n_units) binary patterns ‚àà {-1, +1}\n",
    "        \"\"\"\n",
    "        assert patterns.shape[1] == self.n_units\n",
    "        assert np.all(np.isin(patterns, [-1, 1])), \"Patterns must be in {-1, +1}\"\n",
    "\n",
    "        P = patterns.shape[0]\n",
    "        self.J = np.dot(patterns.T, patterns) / P\n",
    "        np.fill_diagonal(self.J, 0)  # Remove self-connections\n",
    "\n",
    "    def energy(self, state):\n",
    "        \"\"\"\n",
    "        Compute energy of a given state: E = -1/2 * s.T @ J @ s\n",
    "        \"\"\"\n",
    "        assert state.shape[0] == self.n_units\n",
    "        return -0.5 * np.dot(state, np.dot(self.J, state))\n",
    "\n",
    "    def update_async(self, state, n_steps=100, tol=1e-5):\n",
    "        \"\"\"\n",
    "        Simulate asynchronous updates until convergence or max steps.\n",
    "        Returns:\n",
    "            - final state (1D array of shape [n_units])\n",
    "            - list of energy values\n",
    "        \"\"\"\n",
    "        state = state.copy()\n",
    "        energy_trace = [self.energy(state)]\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            old_state = state.copy()\n",
    "\n",
    "            # Random order update\n",
    "            for i in np.random.permutation(self.n_units):\n",
    "                h_i = np.dot(self.J[i], state)\n",
    "                state[i] = 1 if h_i >= 0 else -1\n",
    "\n",
    "            energy_trace.append(self.energy(state))\n",
    "\n",
    "            # Convergence check: state doesn't change\n",
    "            if np.allclose(state, old_state, atol=tol):\n",
    "                break\n",
    "\n",
    "        return state, energy_trace\n",
    "\n",
    "    def run_multiple(self, initial_states, n_steps=100, tol=1e-5):\n",
    "        \"\"\"\n",
    "        Run update_async from multiple initial states.\n",
    "        Returns:\n",
    "            - list of final stable states\n",
    "            - list of number of steps to converge\n",
    "            - list of energy traces\n",
    "        \"\"\"\n",
    "        stable_states = []\n",
    "        convergence_steps = []\n",
    "        energy_trajectories = []\n",
    "\n",
    "        for state in initial_states:\n",
    "            final, energy_path = self.update_async(state, n_steps=n_steps, tol=tol)\n",
    "            stable_states.append(final)\n",
    "            convergence_steps.append(len(energy_path))\n",
    "            energy_trajectories.append(energy_path)\n",
    "\n",
    "        return stable_states, convergence_steps, energy_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d8392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hopfield_features(patterns_dict, n_init=20):\n",
    "    \"\"\"\n",
    "    Run Hopfield model per subject and band.\n",
    "    Returns:\n",
    "        features_dict[subject][band] = {\n",
    "            'n_attractors', 'avg_steps', 'avg_energy'\n",
    "        }\n",
    "    \"\"\"\n",
    "    features_dict = {}\n",
    "\n",
    "    for subj_id in patterns_dict:\n",
    "        features_dict[subj_id] = {}\n",
    "        for band in patterns_dict[subj_id]:\n",
    "            patterns = patterns_dict[subj_id][band]\n",
    "\n",
    "            if patterns.shape[0] < 2:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                model = HopfieldNetwork(n_units=patterns.shape[1])\n",
    "                model.train_hebbian(patterns)\n",
    "\n",
    "                # Initial states = random subset of patterns\n",
    "                idx = np.random.choice(patterns.shape[0], size=min(n_init, len(patterns)), replace=False)\n",
    "                init_states = patterns[idx]\n",
    "\n",
    "                stable, steps, energy_traj = model.run_multiple(init_states)\n",
    "\n",
    "                stable = np.array(stable)\n",
    "                unique_attractors = np.unique(stable, axis=0)\n",
    "\n",
    "                features_dict[subj_id][band] = {\n",
    "                    'n_attractors': len(unique_attractors),\n",
    "                    'avg_steps': np.mean(steps),\n",
    "                    'avg_energy': np.mean([model.energy(s) for s in stable])\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {subj_id} - {band} failed: {e}\")\n",
    "                continue\n",
    "\n",
    "    return features_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e41c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_patterns(binary_data, bands=['theta', 'alpha', 'beta', 'gamma'], max_patterns=100):\n",
    "    \"\"\"\n",
    "    Extract binary EEG state patterns for all subjects and bands, ensure {-1, +1}.\n",
    "    Returns:\n",
    "        patterns_dict[subject][band] = (n_patterns, n_channels)\n",
    "    \"\"\"\n",
    "    patterns_dict = {}\n",
    "\n",
    "    for subj_id in binary_data:\n",
    "        patterns_dict[subj_id] = {}\n",
    "        for band in bands:\n",
    "            try:\n",
    "                X = binary_data[subj_id][band].T  # (timepoints, channels)\n",
    "\n",
    "                # Sample max_patterns timepoints\n",
    "                if X.shape[0] > max_patterns:\n",
    "                    idx = np.random.choice(X.shape[0], size=max_patterns, replace=False)\n",
    "                    patterns = X[idx]\n",
    "                else:\n",
    "                    patterns = X\n",
    "\n",
    "                # ‚úÖ Enforce binary ‚Üí bipolar conversion\n",
    "                patterns = np.where(patterns == 0, -1, 1)\n",
    "\n",
    "                # Sanity check\n",
    "                if not np.all(np.isin(patterns, [-1, 1])):\n",
    "                    raise ValueError(\"Pattern conversion failed!\")\n",
    "\n",
    "                patterns_dict[subj_id][band] = patterns\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Skipping subject {subj_id}, band {band}: {e}\")\n",
    "    \n",
    "    return patterns_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05769c90",
   "metadata": {},
   "source": [
    "# should be deleted\n",
    "\n",
    "def sample_patterns_from_ising(ising_dict, bands=['theta', 'alpha', 'beta', 'gamma'], n_samples=100):\n",
    "    \"\"\"\n",
    "    Sample patterns from Ising model parameters using Gibbs sampling.\n",
    "    Returns:\n",
    "        patterns_dict[subject][band] = (n_samples, n_channels)\n",
    "    \"\"\"\n",
    "    def gibbs_sample(h, J, n_samples):\n",
    "        n = len(h)\n",
    "        state = np.random.choice([-1, 1], size=n)\n",
    "        samples = []\n",
    "\n",
    "        for _ in range(n_samples * 10):  # Burn-in and oversampling\n",
    "            for i in range(n):\n",
    "                field = h[i] + np.dot(J[i], state) - J[i, i] * state[i]\n",
    "                prob = 1 / (1 + np.exp(-2 * field * state[i]))\n",
    "                state[i] = 1 if np.random.rand() < prob else -1\n",
    "            if len(samples) < n_samples:\n",
    "                samples.append(state.copy())\n",
    "\n",
    "        return np.array(samples)\n",
    "\n",
    "    patterns_dict = {}\n",
    "\n",
    "    for subj_id in ising_dict:\n",
    "        patterns_dict[subj_id] = {}\n",
    "        for band in bands:\n",
    "            try:\n",
    "                h = ising_dict[subj_id][band]['h']\n",
    "                J = ising_dict[subj_id][band]['J']\n",
    "                patterns = gibbs_sample(h, J, n_samples=n_samples)\n",
    "                patterns_dict[subj_id][band] = patterns\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Skipping subject {subj_id}, band {band}: {e}\")\n",
    "    \n",
    "    return patterns_dict\n",
    "\n",
    "patterns_rest = sample_patterns_from_ising(ising_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f65d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_task = extract_all_patterns(binary_task) # patterns_rest = extract_all_patterns(binary_rest))\n",
    "patterns_rest = extract_all_patterns(binary_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864db5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(patterns_rest['12']['alpha'])  # should be array([-1, 1])\n",
    "np.unique(patterns_task['12']['alpha']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a0652",
   "metadata": {},
   "outputs": [],
   "source": [
    "hopfield_feats_task = hopfield_features(patterns_task)\n",
    "hopfield_feats_rest = hopfield_features(patterns_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def groupwise_hopfield_features(hopfield_feats, good_ids, bad_ids, bands=['theta', 'alpha', 'beta', 'gamma', 'broadband']):\n",
    "    \"\"\"\n",
    "    Organize Hopfield features for group comparison.\n",
    "    Returns:\n",
    "        features[group][band][metric] = list of values across subjects\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        'good': defaultdict(lambda: defaultdict(list)),\n",
    "        'bad': defaultdict(lambda: defaultdict(list))\n",
    "    }\n",
    "\n",
    "    for group, ids in [('good', good_ids), ('bad', bad_ids)]:\n",
    "        for subj_id in ids:\n",
    "            subj_str = str(subj_id)  # subject IDs are strings in hopfield_feats\n",
    "            if subj_str not in hopfield_feats:\n",
    "                continue\n",
    "            for band in bands:\n",
    "                if band not in hopfield_feats[subj_str]:\n",
    "                    continue\n",
    "\n",
    "                feat_dict = hopfield_feats[subj_str][band]\n",
    "                for metric, value in feat_dict.items():\n",
    "                    # Include only scalar numeric values (ignore arrays, etc.)\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        features[group][band][metric].append(value)\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b61fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good/Bad group definitions using numeric keys as strings\n",
    "bad_counters = ['0', '4', '6', '9', '10', '14', '19', '21', '22', '30']\n",
    "good_counters = ['1', '2', '3', '5', '7', '8', '11', '12', '13', '15', '16', '17',\n",
    "                 '18', '20', '23', '24', '25', '26', '27', '28', '29', '31', '32',\n",
    "                 '33', '34', '35']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e282c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_by_group = groupwise_hopfield_features(\n",
    "    hopfield_feats_rest,\n",
    "    good_ids=good_counters,\n",
    "    bad_ids=bad_counters\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67e73ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_by_group['good']['alpha']['n_attractors']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6729f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def compare_group_feature(features_by_group, band='alpha', metric='n_attractors'):\n",
    "    good = features_by_group['good'][band][metric]\n",
    "    bad = features_by_group['bad'][band][metric]\n",
    "\n",
    "    if len(good) < 2 or len(bad) < 2:\n",
    "        print(\"Not enough data to compare.\")\n",
    "        return\n",
    "\n",
    "    t, p = ttest_ind(good, bad, equal_var=False)\n",
    "    print(f\"üìä {metric.upper()} | Band: {band.upper()}\")\n",
    "    print(f\"   ‚û§ Good mean: {np.mean(good):.2f}\")\n",
    "    print(f\"   ‚û§ Bad  mean: {np.mean(bad):.2f}\")\n",
    "    print(f\"   ‚û§ t = {t:.2f}, p = {p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_group_feature(features_by_group, band='alpha', metric='n_attractors')\n",
    "compare_group_feature(features_by_group, band='beta', metric='avg_steps')\n",
    "compare_group_feature(features_by_group, band='theta', metric='avg_energy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hopfield_metrics = [\n",
    "    'n_attractors',\n",
    "    'avg_steps',\n",
    "    'avg_energy',\n",
    "]\n",
    "\n",
    "for band in ['theta', 'alpha', 'beta', 'gamma', 'broadband']:\n",
    "    for metric in hopfield_metrics:\n",
    "        try:\n",
    "            compare_group_feature(features_by_group, band, metric)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed comparison for {metric} in {band}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cc67e6",
   "metadata": {},
   "source": [
    "### more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23393621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.stats import entropy\n",
    "from itertools import combinations\n",
    "\n",
    "def hopfield_features(patterns_all_subjects, n_init=100):\n",
    "    \"\"\"\n",
    "    Computes Hopfield features across all subjects and bands.\n",
    "    Input:\n",
    "        patterns_all_subjects: {subject_id: {band: binary array [time, channels]}}\n",
    "    Returns:\n",
    "        Dictionary of features per subject per band\n",
    "    \"\"\"\n",
    "    features = defaultdict(dict)\n",
    "\n",
    "    for subj_id, band_dict in patterns_all_subjects.items():\n",
    "        for band, patterns in band_dict.items():\n",
    "            try:\n",
    "                # Convert {0,1} ‚Üí {-1,+1} if needed\n",
    "                if np.any(patterns == 0):\n",
    "                    patterns = np.where(patterns == 0, -1, 1)\n",
    "\n",
    "                n_patterns, n_units = patterns.shape\n",
    "                hopfield = HopfieldNetwork(n_units)\n",
    "                hopfield.train_hebbian(patterns)\n",
    "\n",
    "                # Run dynamics from n_init random states\n",
    "                initial_states = np.random.choice([-1, 1], size=(n_init, n_units))\n",
    "                stable_states, steps_list, energy_traces = hopfield.run_multiple(initial_states)\n",
    "\n",
    "                # Convert states to tuples for hashing\n",
    "                attractors = {}\n",
    "                energies = []\n",
    "                for s in stable_states:\n",
    "                    key = tuple(s)\n",
    "                    attractors[key] = attractors.get(key, 0) + 1\n",
    "                    energies.append(hopfield.energy(s))\n",
    "\n",
    "                basin_sizes = list(attractors.values())\n",
    "                total_basins = sum(basin_sizes)\n",
    "\n",
    "                # Energy differences\n",
    "                energy_gaps = []\n",
    "                for a1, a2 in combinations(attractors.keys(), 2):\n",
    "                    e1 = hopfield.energy(np.array(a1))\n",
    "                    e2 = hopfield.energy(np.array(a2))\n",
    "                    energy_gaps.append(abs(e1 - e2))\n",
    "\n",
    "                features[subj_id][band] = {\n",
    "                    'n_attractors': len(attractors),\n",
    "                    'avg_steps': np.mean(steps_list) if steps_list else np.nan,\n",
    "                    'avg_energy': np.mean(energies) if energies else np.nan,\n",
    "                    'mean_basin_size': np.mean(basin_sizes) if len(basin_sizes) > 0 else np.nan,\n",
    "                    'basin_entropy': entropy(np.array(basin_sizes) / total_basins) if total_basins > 0 else np.nan,\n",
    "                    'min_energy_gap': np.min(energy_gaps) if len(energy_gaps) > 0 else np.nan,\n",
    "                    'avg_energy_gap': np.mean(energy_gaps) if len(energy_gaps) > 0 else np.nan,\n",
    "                    'load_ratio': patterns.shape[0] / patterns.shape[1]  # ‚úÖ Always computable\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {subj_id} - {band} failed: {e}\")\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032ef17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.stats import entropy\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm  # ‚úÖ Import tqdm\n",
    "\n",
    "def hopfield_features(patterns_all_subjects, n_init=100):\n",
    "    \"\"\"\n",
    "    Computes Hopfield features across all subjects and bands.\n",
    "    Input:\n",
    "        patterns_all_subjects: {subject_id: {band: binary array [time, channels]}}\n",
    "    Returns:\n",
    "        Dictionary of features per subject per band\n",
    "    \"\"\"\n",
    "    features = defaultdict(dict)\n",
    "\n",
    "    for subj_id, band_dict in tqdm(patterns_all_subjects.items(), desc=\"üß† Subjects\"):\n",
    "        for band, patterns in tqdm(band_dict.items(), desc=f\"  üéß Bands ({subj_id})\", leave=False):\n",
    "            try:\n",
    "                # Convert {0,1} ‚Üí {-1,+1} if needed\n",
    "                if np.any(patterns == 0):\n",
    "                    patterns = np.where(patterns == 0, -1, 1)\n",
    "\n",
    "                n_patterns, n_units = patterns.shape\n",
    "                hopfield = HopfieldNetwork(n_units)\n",
    "                hopfield.train_hebbian(patterns)\n",
    "\n",
    "                # Run dynamics from n_init random states\n",
    "                initial_states = np.random.choice([-1, 1], size=(n_init, n_units))\n",
    "                stable_states, steps_list, energy_traces = hopfield.run_multiple(initial_states)\n",
    "\n",
    "                # Convert states to tuples for hashing\n",
    "                attractors = {}\n",
    "                energies = []\n",
    "                for s in stable_states:\n",
    "                    key = tuple(s)\n",
    "                    attractors[key] = attractors.get(key, 0) + 1\n",
    "                    energies.append(hopfield.energy(s))\n",
    "\n",
    "                basin_sizes = list(attractors.values())\n",
    "                total_basins = sum(basin_sizes)\n",
    "\n",
    "                # Energy differences\n",
    "                energy_gaps = []\n",
    "                for a1, a2 in combinations(attractors.keys(), 2):\n",
    "                    e1 = hopfield.energy(np.array(a1))\n",
    "                    e2 = hopfield.energy(np.array(a2))\n",
    "                    energy_gaps.append(abs(e1 - e2))\n",
    "\n",
    "                features[subj_id][band] = {\n",
    "                    'n_attractors': len(attractors),\n",
    "                    'avg_steps': np.mean(steps_list) if steps_list else np.nan,\n",
    "                    'avg_energy': np.mean(energies) if energies else np.nan,\n",
    "                    'mean_basin_size': np.mean(basin_sizes) if len(basin_sizes) > 0 else np.nan,\n",
    "                    'basin_entropy': entropy(np.array(basin_sizes) / total_basins) if total_basins > 0 else np.nan,\n",
    "                    'min_energy_gap': np.min(energy_gaps) if energy_gaps else np.nan,\n",
    "                    'avg_energy_gap': np.mean(energy_gaps) if energy_gaps else np.nan,\n",
    "                    'load_ratio': patterns.shape[0] / patterns.shape[1]\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {subj_id} - {band} failed: {e}\")\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_rest = extract_all_patterns(binary_rest)\n",
    "hopfield_feats_rest = hopfield_features(patterns_rest)\n",
    "patterns_task = extract_all_patterns(binary_task)\n",
    "hopfield_feats_task = hopfield_features(patterns_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def groupwise_hopfield_features(hopfield_feats, good_ids, bad_ids, bands=['theta', 'alpha', 'beta', 'gamma', 'broadband']):\n",
    "    features = {\n",
    "        'good': defaultdict(lambda: defaultdict(list)),\n",
    "        'bad': defaultdict(lambda: defaultdict(list))\n",
    "    }\n",
    "\n",
    "    for group, ids in [('good', good_ids), ('bad', bad_ids)]:\n",
    "        for subj_id in ids:\n",
    "            sid = str(subj_id)\n",
    "            if sid not in hopfield_feats:\n",
    "                print(f\"‚ö†Ô∏è Missing subject: {sid}\")\n",
    "                continue\n",
    "\n",
    "            for band in bands:\n",
    "                if band not in hopfield_feats[sid]:\n",
    "                    print(f\"‚ö†Ô∏è Missing band: {band} for subject {sid}\")\n",
    "                    continue\n",
    "\n",
    "                for metric, value in hopfield_feats[sid][band].items():\n",
    "                    # ‚úÖ Sanity check: skip None, NaN, or invalid types\n",
    "                    if value is None:\n",
    "                        print(f\"‚ö†Ô∏è Skipping {metric} for subject {sid} (None)\")\n",
    "                        continue\n",
    "                    if isinstance(value, (float, int, np.float64, np.int64)):\n",
    "                        if np.isnan(value):\n",
    "                            print(f\"‚ö†Ô∏è Skipping {metric} for subject {sid} (NaN)\")\n",
    "                            continue\n",
    "                        features[group][band][metric].append(float(value))\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è Invalid type for {metric} in subject {sid}: {type(value)}\")\n",
    "                        continue\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3fc433",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_by_group_rest = groupwise_hopfield_features(hopfield_feats_rest, good_counters, bad_counters)\n",
    "features_by_group_task = groupwise_hopfield_features(hopfield_feats_task, good_counters, bad_counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"binarized/binary_task.pkl\", \"rb\") as f:\n",
    "    hopfield_binaries_task = pickle.load(f)\n",
    "with open(\"binarized/binary_rest.pkl\", \"rb\") as f:\n",
    "    hopfield_binaries_rest = pickle.load(f)\n",
    "features_by_group_rest = hopfield_features(hopfield_binaries_rest, n_init=100)\n",
    "features_by_group_task = hopfield_features(hopfield_binaries_task, n_init=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7cec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hopfield_metrics = [\n",
    "    'n_attractors', 'avg_steps', 'avg_energy',\n",
    "    'mean_basin_size', 'basin_entropy',\n",
    "    'min_energy_gap', 'avg_energy_gap', 'load_ratio'\n",
    "]\n",
    "\n",
    "for band in ['theta', 'alpha', 'beta', 'gamma']:\n",
    "    for metric in all_hopfield_metrics:\n",
    "        compare_group_feature(features_by_group_task, band, metric)\n",
    "\n",
    "for band in ['theta', 'alpha', 'beta', 'gamma']:\n",
    "    for metric in all_hopfield_metrics:\n",
    "        compare_group_feature(features_by_group_rest, band, metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4522f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from scipy.stats import entropy\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# === Hopfield network implementation ===\n",
    "class HopfieldNetwork:\n",
    "    def __init__(self, n_units):\n",
    "        self.n_units = n_units\n",
    "        self.weights = np.zeros((n_units, n_units))\n",
    "\n",
    "    def train_hebbian(self, patterns):\n",
    "        self.weights = np.dot(patterns.T, patterns)\n",
    "        np.fill_diagonal(self.weights, 0)\n",
    "\n",
    "    def energy(self, state):\n",
    "        return -0.5 * np.dot(state, np.dot(self.weights, state))\n",
    "\n",
    "    def run(self, state, max_iter=100):\n",
    "        prev = state.copy()\n",
    "        for _ in range(max_iter):\n",
    "            for i in range(self.n_units):\n",
    "                s = np.dot(self.weights[i], prev)\n",
    "                prev[i] = 1 if s >= 0 else -1\n",
    "        return prev\n",
    "\n",
    "    def run_multiple(self, initial_states):\n",
    "        stable_states = []\n",
    "        steps_list = []\n",
    "        energy_traces = []\n",
    "\n",
    "        for init in initial_states:\n",
    "            state = init.copy()\n",
    "            steps = 0\n",
    "            trace = []\n",
    "\n",
    "            for _ in range(100):\n",
    "                trace.append(self.energy(state))\n",
    "                new_state = self.run(state)\n",
    "                steps += 1\n",
    "                if np.array_equal(new_state, state):\n",
    "                    break\n",
    "                state = new_state\n",
    "\n",
    "            stable_states.append(state)\n",
    "            steps_list.append(steps)\n",
    "            energy_traces.append(trace)\n",
    "\n",
    "        return stable_states, steps_list, energy_traces\n",
    "\n",
    "# === Feature extraction from Hopfield dynamics ===\n",
    "def hopfield_features(patterns_all_subjects, n_init=100, max_pairs=5000):\n",
    "    features = defaultdict(dict)\n",
    "\n",
    "    for subj_id, band_dict in tqdm(patterns_all_subjects.items(), desc=\"üß† Subjects\"):\n",
    "        for band, patterns in tqdm(band_dict.items(), desc=f\"  üéß Bands ({subj_id})\", leave=False):\n",
    "            try:\n",
    "                if np.any(patterns == 0):\n",
    "                    patterns = np.where(patterns == 0, -1, 1)\n",
    "\n",
    "                n_patterns, n_units = patterns.shape\n",
    "                hopfield = HopfieldNetwork(n_units)\n",
    "                hopfield.train_hebbian(patterns)\n",
    "\n",
    "                initial_states = np.random.choice([-1, 1], size=(n_init, n_units))\n",
    "                stable_states, steps_list, energy_traces = hopfield.run_multiple(initial_states)\n",
    "\n",
    "                attractors = {}\n",
    "                energies = []\n",
    "\n",
    "                for s in stable_states:\n",
    "                    key = tuple(s)\n",
    "                    attractors[key] = attractors.get(key, 0) + 1\n",
    "                    energies.append(hopfield.energy(s))\n",
    "\n",
    "                basin_sizes = list(attractors.values())\n",
    "                total_basins = sum(basin_sizes)\n",
    "\n",
    "                # Sample energy gaps\n",
    "                attractor_keys = list(attractors.keys())\n",
    "                sampled_pairs = list(combinations(attractor_keys, 2))\n",
    "                if len(sampled_pairs) > max_pairs:\n",
    "                    sampled_pairs = random.sample(sampled_pairs, max_pairs)\n",
    "\n",
    "                energy_gaps = [\n",
    "                    abs(hopfield.energy(np.array(a1)) - hopfield.energy(np.array(a2)))\n",
    "                    for a1, a2 in sampled_pairs\n",
    "                ]\n",
    "\n",
    "                features[subj_id][band] = {\n",
    "                    'n_attractors': len(attractors),\n",
    "                    'avg_steps': np.mean(steps_list) if steps_list else np.nan,\n",
    "                    'avg_energy': np.mean(energies) if energies else np.nan,\n",
    "                    'mean_basin_size': np.mean(basin_sizes) if basin_sizes else np.nan,\n",
    "                    'basin_entropy': entropy(np.array(basin_sizes) / total_basins) if total_basins > 0 else np.nan,\n",
    "                    'min_energy_gap': np.min(energy_gaps) if energy_gaps else np.nan,\n",
    "                    'avg_energy_gap': np.mean(energy_gaps) if energy_gaps else np.nan,\n",
    "                    'load_ratio': n_patterns / n_units\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {subj_id} - {band} failed: {e}\")\n",
    "\n",
    "    return features\n",
    "\n",
    "# === Groupwise summarization ===\n",
    "def groupwise_hopfield_features(hopfield_feats, good_ids, bad_ids, bands=['theta', 'alpha', 'beta', 'gamma', 'broadband']):\n",
    "    features = {\n",
    "        'good': defaultdict(lambda: defaultdict(list)),\n",
    "        'bad': defaultdict(lambda: defaultdict(list))\n",
    "    }\n",
    "\n",
    "    for group, ids in [('good', good_ids), ('bad', bad_ids)]:\n",
    "        for subj_id in ids:\n",
    "            sid = str(subj_id)\n",
    "            if sid not in hopfield_feats:\n",
    "                print(f\"‚ö†Ô∏è Missing subject: {sid}\")\n",
    "                continue\n",
    "\n",
    "            for band in bands:\n",
    "                if band not in hopfield_feats[sid]:\n",
    "                    print(f\"‚ö†Ô∏è Missing band: {band} for subject {sid}\")\n",
    "                    continue\n",
    "\n",
    "                for metric, value in hopfield_feats[sid][band].items():\n",
    "                    if value is None or (isinstance(value, float) and np.isnan(value)):\n",
    "                        continue\n",
    "                    features[group][band][metric].append(float(value))\n",
    "\n",
    "    return features\n",
    "\n",
    "# === Feature comparison ===\n",
    "def compare_group_feature(grouped_feats, band, metric):\n",
    "    from scipy.stats import ttest_ind\n",
    "    good = grouped_feats['good'][band][metric]\n",
    "    bad = grouped_feats['bad'][band][metric]\n",
    "    if not good or not bad:\n",
    "        print(f\"‚ö†Ô∏è Insufficient data for {band} - {metric}\")\n",
    "        return\n",
    "    tstat, pval = ttest_ind(good, bad, equal_var=False)\n",
    "    print(f\"üìä {metric.upper()} ({band}):\")\n",
    "    print(f\"   Good mean: {np.mean(good):.3f} | Bad mean: {np.mean(bad):.3f} | p = {pval:.4f}\\n\")\n",
    "\n",
    "# === Main execution ===\n",
    "if __name__ == \"__main__\":\n",
    "    # Load binarized data\n",
    "    with open(\"binarized/binary_rest.pkl\", \"rb\") as f:\n",
    "        binary_rest = pickle.load(f)\n",
    "    with open(\"binarized/binary_task.pkl\", \"rb\") as f:\n",
    "        binary_task = pickle.load(f)\n",
    "\n",
    "    # Extract patterns\n",
    "    patterns_rest = binary_rest  # Already band-structured\n",
    "    patterns_task = binary_task\n",
    "\n",
    "    # Extract features\n",
    "    hopfield_feats_rest = hopfield_features(patterns_rest, n_init=100)\n",
    "    hopfield_feats_task = hopfield_features(patterns_task, n_init=100)\n",
    "\n",
    "    # Define group labels\n",
    "    #good_counters = [...]  # Fill with list of good subject IDs\n",
    "    #bad_counters = [...]   # Fill with list of bad subject IDs\n",
    "\n",
    "    # Groupwise aggregation\n",
    "    features_by_group_rest = groupwise_hopfield_features(hopfield_feats_rest, good_counters, bad_counters)\n",
    "    features_by_group_task = groupwise_hopfield_features(hopfield_feats_task, good_counters, bad_counters)\n",
    "\n",
    "    # Metrics to evaluate\n",
    "    all_hopfield_metrics = [\n",
    "        'n_attractors', 'avg_steps', 'avg_energy',\n",
    "        'mean_basin_size', 'basin_entropy',\n",
    "        'min_energy_gap', 'avg_energy_gap', 'load_ratio'\n",
    "    ]\n",
    "\n",
    "    print(\"\\n==== TASK COMPARISON ====\")\n",
    "    for band in ['theta', 'alpha', 'beta', 'gamma']:\n",
    "        for metric in all_hopfield_metrics:\n",
    "            compare_group_feature(features_by_group_task, band, metric)\n",
    "\n",
    "    print(\"\\n==== REST COMPARISON ====\")\n",
    "    for band in ['theta', 'alpha', 'beta', 'gamma']:\n",
    "        for metric in all_hopfield_metrics:\n",
    "            compare_group_feature(features_by_group_rest, band, metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1954e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the Hopfield features for ML\n",
    "with open(\"all_features/hopf_features_task.pkl\", \"wb\") as f:\n",
    "    pickle.dump(hopfield_feats_task, f)\n",
    "\n",
    "print(\"‚úÖ Saved Hopfield features to all_features/hopf_features_task.pkl\")\n",
    "\n",
    "\n",
    "# Save the Hopfield features for ML\n",
    "with open(\"all_features/hopf_features_rest.pkl\", \"wb\") as f:\n",
    "    pickle.dump(hopfield_feats_rest, f)\n",
    "\n",
    "print(\"‚úÖ Saved Hopfield features to all_features/hopf_features_rest.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# === Manually entered from your table ===\n",
    "data = {\n",
    "    \"Alpha - Attractor Count\": [2.77, 2.20, 0.1197],\n",
    "    \"Beta - Attractor Count\": [3.38, 2.70, 0.1772],\n",
    "    \"Beta - Basin Entropy\": [1.02, 0.84, 0.1148],\n",
    "    \"Gamma - Average Steps to Convergence\": [4.24, 4.04, 0.0845],\n",
    "    \"Gamma - Mean Basin Size\": [34.21, 25.42, 0.1222],\n",
    "}\n",
    "\n",
    "labels = list(data.keys())\n",
    "good_means = [v[0] for v in data.values()]\n",
    "bad_means = [v[1] for v in data.values()]\n",
    "pvals = [v[2] for v in data.values()]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.45\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width/2, good_means, width, label='Good', color='green')\n",
    "bars2 = ax.bar(x + width/2, bad_means, width, label='Bad', color='red')\n",
    "\n",
    "# Add p-values above bars\n",
    "for i, (g, b, p) in enumerate(zip(good_means, bad_means, pvals)):\n",
    "    y = max(g, b) + 0.6\n",
    "    ax.text(i, y, f\"p = {p:.3f}\", ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Feature Value')\n",
    "#ax.set_title('Hopfield Network Features (Good vs Bad)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c79167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# === Data with clearer formatting ===\n",
    "data = {\n",
    "    r\"$\\alpha$ - Attractor Count\": [2.77, 2.20, 0.1197],\n",
    "    r\"$\\beta$ - Attractor Count\": [3.38, 2.70, 0.1772],\n",
    "    r\"$\\beta$ - Basin Entropy\": [1.02, 0.84, 0.1148],\n",
    "    r\"$\\gamma$ - Avg Steps to Convergence\": [4.24, 4.04, 0.0845],\n",
    "    r\"$\\gamma$ - Mean Basin Size\": [34.21, 25.42, 0.1222],\n",
    "}\n",
    "\n",
    "labels = list(data.keys())\n",
    "good_means = [v[0] for v in data.values()]\n",
    "bad_means = [v[1] for v in data.values()]\n",
    "pvals = [v[2] for v in data.values()]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.4\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width/2, good_means, width, label='Good', color='green')\n",
    "bars2 = ax.bar(x + width/2, bad_means, width, label='Bad', color='crimson')\n",
    "\n",
    "# Annotate p-values above the taller bars\n",
    "for i, (g, b, p) in enumerate(zip(good_means, bad_means, pvals)):\n",
    "    y = max(g, b) + 1\n",
    "    ax.text(i, y, f\"$p$ = {p:.3f}\", ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Format axis\n",
    "ax.set_ylabel('Feature Value', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, rotation=20, ha='right', fontsize=10)\n",
    "\n",
    "# Shift x-tick labels slightly to the right\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_x(tick.get_position()[0] - 3.15)\n",
    "\n",
    "# Optional: adjust spacing between ticks and axis\n",
    "ax.tick_params(axis='x', pad=5)\n",
    "\n",
    "ax.legend(fontsize=10, loc='best')\n",
    "ax.set_title(\"Comparison of Good vs Bad Performers Across EEG-Derived Features\")\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_group_feature_distribution(features_by_group, metric, save_fig=False):\n",
    "    \"\"\"\n",
    "    Plot boxplots of a given Hopfield metric across bands and groups.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for group in ['good', 'bad']:\n",
    "        for band in ['theta', 'alpha', 'beta', 'gamma']:\n",
    "            values = features_by_group[group][band].get(metric, [])\n",
    "            for v in values:\n",
    "                records.append({'Group': group, 'Band': band, 'Value': v})\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.boxplot(x='Band', y='Value', hue='Group', data=df, palette='Set2')\n",
    "    plt.title(f\"Group Comparison for {metric.replace('_', ' ').title()}\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_fig:\n",
    "        plt.savefig(f\"plots/{metric}.png\", dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51c5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in all_hopfield_metrics:\n",
    "    plot_group_feature_distribution(features_by_group, metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1849d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def plot_group_feature_distribution(features_by_group, metric, save_fig=False, out_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Plot boxplots comparing a given Hopfield feature across groups and EEG bands.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for group in ['good', 'bad']:\n",
    "        for band in ['theta', 'alpha', 'beta', 'gamma']:\n",
    "            values = features_by_group[group][band].get(metric, [])\n",
    "            for v in values:\n",
    "                records.append({'Group': group, 'Band': band, 'Value': v})\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"‚ö†Ô∏è No data found for metric: {metric}\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.boxplot(x='Band', y='Value', hue='Group', data=df, palette='Set2')\n",
    "    plt.title(f\"üìä {metric.replace('_', ' ').title()} across EEG Bands\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_fig:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        plt.savefig(f\"{out_dir}/{metric}.png\", dpi=300)\n",
    "        print(f\"‚úÖ Saved: {out_dir}/{metric}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ea9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hopfield_metrics = [\n",
    "    'n_attractors', 'avg_steps', 'avg_energy',\n",
    "    'mean_basin_size', 'basin_entropy',\n",
    "    'min_energy_gap', 'avg_energy_gap', 'load_ratio'\n",
    "]\n",
    "\n",
    "for metric in all_hopfield_metrics:\n",
    "    plot_group_feature_distribution(features_by_group, metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5e6d71",
   "metadata": {},
   "source": [
    "## Classification using Hopfield Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def prepare_classification_data(features_by_group, bands=['theta', 'alpha', 'beta', 'gamma']):\n",
    "    \"\"\"\n",
    "    Create feature matrix X and label vector y.\n",
    "    Each row = one subject √ó band\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for group_label, group in enumerate(['good', 'bad']):  # good ‚Üí 0, bad ‚Üí 1\n",
    "        for subj_idx in range(len(features_by_group[group]['theta']['n_attractors'])):\n",
    "            for band in bands:\n",
    "                try:\n",
    "                    row = {\n",
    "                        'label': group_label,\n",
    "                        'band': band,\n",
    "                    }\n",
    "                    for metric in all_hopfield_metrics:\n",
    "                        values = features_by_group[group][band].get(metric, [])\n",
    "                        if subj_idx < len(values):\n",
    "                            row[metric] = values[subj_idx]\n",
    "                        else:\n",
    "                            row[metric] = np.nan\n",
    "                    rows.append(row)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error processing subject {subj_idx} in {group}-{band}: {e}\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = prepare_classification_data(features_by_group)\n",
    "df = df.dropna()  # remove any rows with missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a28345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Encode band as one-hot\n",
    "df_encoded = pd.get_dummies(df, columns=['band'])\n",
    "\n",
    "X = df_encoded.drop('label', axis=1).values\n",
    "y = df_encoded['label'].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"üìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"üìâ Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feat_names = df_encoded.drop('label', axis=1).columns\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(feat_names, importances)\n",
    "plt.title(\"üéØ Feature Importance\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc90e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_res, y_res)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9abfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa24aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de47a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"üìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nüìâ Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf35ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, display_labels=['Good', 'Bad'])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e1fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------\n",
    "# STEP 1: Prepare Data\n",
    "# ----------------------\n",
    "\n",
    "# Assuming you already have your features and labels:\n",
    "# X = features (as DataFrame or array)\n",
    "# y = labels (0 for good, 1 for bad)\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n",
    "\n",
    "# ----------------------\n",
    "# STEP 2: Train Classifier\n",
    "# ----------------------\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------\n",
    "# STEP 3: Evaluate\n",
    "# ----------------------\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nüìâ Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# ----------------------\n",
    "# STEP 4: Feature Importance\n",
    "# ----------------------\n",
    "\n",
    "# Feature names\n",
    "feature_names = X.columns if isinstance(X, pd.DataFrame) else [f'feature_{i}' for i in range(X.shape[1])]\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# Create Series and plot\n",
    "feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nüîé Top 10 Important Features:\")\n",
    "print(feat_imp.head(10))\n",
    "\n",
    "# Plot top 10\n",
    "feat_imp.head(10).plot(kind='barh', title='Top 10 Feature Importances', figsize=(8, 5))\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c81b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_hopfield_features(hopfield_feats, subject_ids=None):\n",
    "    \"\"\"\n",
    "    Flatten nested Hopfield feature dictionary into a tabular format.\n",
    "    Returns:\n",
    "        - X: DataFrame of shape (n_subjects, n_features)\n",
    "        - y: Labels (0=bad, 1=good)\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    all_subjects = list(hopfield_feats.keys()) if subject_ids is None else subject_ids\n",
    "    bands = ['theta', 'alpha', 'beta', 'gamma']\n",
    "    features = [\n",
    "        'n_attractors', 'avg_steps', 'avg_energy',\n",
    "        'mean_basin_size', 'basin_entropy',\n",
    "        'min_energy_gap', 'avg_energy_gap', 'load_ratio'\n",
    "    ]\n",
    "\n",
    "    for sid in all_subjects:\n",
    "        row = {}\n",
    "        if sid not in hopfield_feats:\n",
    "            continue\n",
    "        for band in bands:\n",
    "            if band not in hopfield_feats[sid]:\n",
    "                continue\n",
    "            for feat in features:\n",
    "                value = hopfield_feats[sid][band].get(feat, np.nan)\n",
    "                row[f\"{band}_{feat}\"] = value\n",
    "        row[\"subject_id\"] = sid\n",
    "        row[\"label\"] = 1 if sid in good_counters else 0  # good = 1, bad = 0\n",
    "        all_rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(all_rows).set_index(\"subject_id\")\n",
    "    df = df.dropna(axis=0)  # Drop subjects with missing features\n",
    "    return df.drop(columns=\"label\"), df[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b6077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_X_y_from_features(features_by_group, metrics, bands):\n",
    "    \"\"\"\n",
    "    Convert nested feature dictionary into feature matrix X and labels y.\n",
    "    Output:\n",
    "        - X: shape (n_samples, n_features)\n",
    "        - y: binary labels: 0=good, 1=bad\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    feature_names = []\n",
    "\n",
    "    for group_label, group in enumerate(['good', 'bad']):\n",
    "        for subj_idx in range(len(features_by_group[group][bands[0]][metrics[0]])):\n",
    "            feature_vector = []\n",
    "            for band in bands:\n",
    "                for metric in metrics:\n",
    "                    try:\n",
    "                        value = features_by_group[group][band][metric][subj_idx]\n",
    "                    except IndexError:\n",
    "                        value = np.nan\n",
    "                    feature_vector.append(value)\n",
    "            X.append(feature_vector)\n",
    "            y.append(group_label)\n",
    "\n",
    "    feature_names = [f\"{band}_{metric}\" for band in bands for metric in metrics]\n",
    "    return np.array(X), np.array(y), feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2045f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your desired bands and metrics\n",
    "band_list = ['theta', 'alpha', 'beta', 'gamma']\n",
    "metric_list = [\n",
    "    'n_attractors', 'avg_steps', 'avg_energy',\n",
    "    'mean_basin_size', 'basin_entropy',\n",
    "    'min_energy_gap', 'avg_energy_gap', 'load_ratio'\n",
    "]\n",
    "\n",
    "# Build X and y\n",
    "X, y, feature_names = build_X_y_from_features(features_by_group, metric_list, band_list)\n",
    "\n",
    "# Now build the DataFrame\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, stratify=y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train model\n",
    "clf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance\n",
    "importances = clf.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=X_df.columns).sort_values(ascending=False)\n",
    "\n",
    "# Plot\n",
    "feat_imp.head(10).plot(kind='barh', title='Top 10 Feature Importances', figsize=(8, 5))\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ff9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def plot_attractors_pca(hopfield_feats, patterns_all, subject_id='12', band='alpha', n_init=100):\n",
    "    \"\"\"\n",
    "    Run Hopfield dynamics and plot final attractors in PCA space.\n",
    "    \"\"\"\n",
    "    patterns = patterns_all[subject_id][band]\n",
    "    n_patterns, n_units = patterns.shape\n",
    "\n",
    "    # Train the Hopfield model\n",
    "    model = HopfieldNetwork(n_units)\n",
    "    model.train_hebbian(patterns)\n",
    "\n",
    "    # Run dynamics from random initial states\n",
    "    init_states = np.random.choice([-1, 1], size=(n_init, n_units))\n",
    "    stable_states, _, _ = model.run_multiple(init_states)\n",
    "\n",
    "    # Convert to array and count attractors\n",
    "    stable_states = np.array(stable_states)\n",
    "    attractor_tuples = [tuple(s) for s in stable_states]\n",
    "    counts = Counter(attractor_tuples)\n",
    "    unique_states = np.array(list(counts.keys()))\n",
    "    sizes = np.array(list(counts.values()))\n",
    "\n",
    "    # PCA projection to 2D\n",
    "    pca = PCA(n_components=2)\n",
    "    proj = pca.fit_transform(unique_states)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    scatter = plt.scatter(proj[:, 0], proj[:, 1], s=sizes*10, c=sizes, cmap='viridis', edgecolor='k')\n",
    "    plt.colorbar(scatter, label=\"Basin Size\")\n",
    "    plt.title(f\"PCA of Attractors - Subject {subject_id} | {band.upper()}\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5910057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def plot_basin_histogram(hopfield_feats, patterns_dict, subject_id, band):\n",
    "    patterns = patterns_dict[subject_id][band]\n",
    "    model = HopfieldNetwork(n_units=patterns.shape[1])\n",
    "    model.train_hebbian(patterns)\n",
    "\n",
    "    stable_states, _, _ = model.run_multiple(patterns)\n",
    "    attractors = [tuple(s) for s in stable_states]\n",
    "    basin_sizes = Counter(attractors).values()\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(basin_sizes, bins=range(1, max(basin_sizes)+2), edgecolor='black')\n",
    "    plt.title(f\"Basin Sizes | Subject {subject_id}, Band: {band}\")\n",
    "    plt.xlabel(\"Size of Basin\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_energy_trajectories(hopfield_feats, patterns_all, subject_id='12', band='alpha'):\n",
    "    \"\"\"\n",
    "    Plot energy trajectories for a given subject and band.\n",
    "    \"\"\"\n",
    "    patterns = patterns_all[subject_id][band]\n",
    "    n_patterns, n_units = patterns.shape\n",
    "\n",
    "    # Re-train Hopfield network\n",
    "    model = HopfieldNetwork(n_units)\n",
    "    model.train_hebbian(patterns)\n",
    "\n",
    "    # Run dynamics\n",
    "    init_states = patterns[:10]  # or random subset\n",
    "    _, _, energy_traces = model.run_multiple(init_states)\n",
    "\n",
    "    # Plot energy over time for each run\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for traj in energy_traces:\n",
    "        plt.plot(traj, alpha=0.6)\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Energy\")\n",
    "    plt.title(f\"Energy Trajectories - Subject {subject_id} | {band.upper()}\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4271ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_local_curvature(hopfield_model, n_units):\n",
    "    \"\"\"\n",
    "    Compute discrete curvature for each binary state in {-1, +1}^n.\n",
    "    Curvature = sum of energy differences between a state and its Hamming neighbors.\n",
    "\n",
    "    Returns:\n",
    "        state_energy: list of energies\n",
    "        curvature: list of curvature values (same order as states)\n",
    "    \"\"\"\n",
    "    all_states = all_binary_states(n_units)  # shape: (2^n, n)\n",
    "    state_energy = []\n",
    "    curvature = []\n",
    "\n",
    "    for i, s in enumerate(all_states):\n",
    "        e_s = hopfield_model.energy(s)\n",
    "        state_energy.append(e_s)\n",
    "\n",
    "        # Hamming neighbors\n",
    "        neighbors = []\n",
    "        for j in range(n_units):\n",
    "            neighbor = s.copy()\n",
    "            neighbor[j] *= -1\n",
    "            neighbors.append(neighbor)\n",
    "\n",
    "        energy_diffs = [hopfield_model.energy(n) - e_s for n in neighbors]\n",
    "        kappa = sum(energy_diffs)\n",
    "        curvature.append(kappa)\n",
    "\n",
    "    return np.array(state_energy), np.array(curvature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b9d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load patterns for subject '12' and band 'alpha'\n",
    "patterns = patterns_rest['12']['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee0548",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subj in patterns_rest:\n",
    "    for band in patterns_rest[subj]:\n",
    "        print(subj, band, patterns_rest[subj][band].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = patterns_rest['29']['theta']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57236223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = HopfieldNetwork(n_units=patterns.shape[1])\n",
    "model.train_hebbian(patterns)\n",
    "\n",
    "# Compute all binary states\n",
    "states = all_binary_states(patterns.shape[1])  # (524288, 19)\n",
    "\n",
    "# Compute energy and curvature\n",
    "state_energy = np.array([model.energy(s) for s in states])\n",
    "\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.sparse import csgraph\n",
    "\n",
    "def compute_local_curvature(model, n_units):\n",
    "    states = all_binary_states(n_units)\n",
    "    energy = np.array([model.energy(s) for s in states])\n",
    "    curvature = []\n",
    "    for i, s in enumerate(states):\n",
    "        neighbors = [s ^ (1 << j) for j in range(n_units)]  # Hamming neighbors\n",
    "        e0 = energy[i]\n",
    "        en = [model.energy(states[j]) for j in neighbors]\n",
    "        lap = sum(en) - len(neighbors) * e0\n",
    "        curvature.append(lap)\n",
    "    return np.array(curvature)\n",
    "\n",
    "# Optional: downsample for faster processing\n",
    "subset_idx = np.random.choice(len(states), size=3000, replace=False)\n",
    "states_sample = states[subset_idx]\n",
    "energy_sample = state_energy[subset_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c702cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "xy = pca.fit_transform(states_sample)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(xy[:, 0], xy[:, 1], c=energy_sample, cmap='viridis', s=10)\n",
    "plt.colorbar(label=\"Energy\")\n",
    "plt.title(\"Hopfield Energy Landscape (PCA view)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcec19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "def energy_landscape_features(patterns_dict, max_states=2**19):\n",
    "    features = {}\n",
    "    \n",
    "    for subj_id in tqdm(patterns_dict):\n",
    "        features[subj_id] = {}\n",
    "        for band in patterns_dict[subj_id]:\n",
    "            patterns = patterns_dict[subj_id][band]\n",
    "            n_units = patterns.shape[1]\n",
    "\n",
    "            try:\n",
    "                # Train Hopfield\n",
    "                model = HopfieldNetwork(n_units)\n",
    "                model.train_hebbian(patterns)\n",
    "\n",
    "                # All states\n",
    "                states = all_binary_states(n_units)\n",
    "                energies = np.array([model.energy(s) for s in states])\n",
    "\n",
    "                # PCA reduction\n",
    "                pca = PCA(n_components=2)\n",
    "                coords = pca.fit_transform(states)\n",
    "\n",
    "                # Store summary\n",
    "                features[subj_id][band] = {\n",
    "                    'mean_energy': float(np.mean(energies)),\n",
    "                    'min_energy': float(np.min(energies)),\n",
    "                    'max_energy': float(np.max(energies)),\n",
    "                    'energy_std': float(np.std(energies)),\n",
    "                    'energy_range': float(np.ptp(energies))\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {subj_id} {band} failed: {e}\")\n",
    "                continue\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6ac36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "landscape_feats_rest = energy_landscape_features(patterns_rest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f365eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def groupwise_energy_features(feats, good_ids, bad_ids, bands, metrics):\n",
    "    grouped = {'good': defaultdict(list), 'bad': defaultdict(list)}\n",
    "\n",
    "    for group, ids in [('good', good_ids), ('bad', bad_ids)]:\n",
    "        for sid in ids:\n",
    "            sid = str(sid)\n",
    "            if sid not in feats:\n",
    "                continue\n",
    "            for band in bands:\n",
    "                if band in feats[sid]:\n",
    "                    for metric in metrics:\n",
    "                        val = feats[sid][band].get(metric, None)\n",
    "                        if val is not None and not np.isnan(val):\n",
    "                            grouped[group][f\"{band}_{metric}\"].append(val)\n",
    "    return grouped\n",
    "\n",
    "# Define groups\n",
    "good_ids = ['1','2','3','5','7','8','11','12','13','15','16','17','18','20','23','24','25','26','27','28','29','31','32','33','34','35']\n",
    "bad_ids  = ['0','4','6','9','10','14','19','21','22','30']\n",
    "\n",
    "metrics = ['mean_energy', 'min_energy', 'max_energy', 'energy_std', 'energy_range']\n",
    "bands = ['theta', 'alpha', 'beta', 'gamma']\n",
    "\n",
    "grouped_energy_feats = groupwise_energy_features(landscape_feats_rest, good_ids, bad_ids, bands, metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93542351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_grouped_features(grouped_feats):\n",
    "    for metric in grouped_feats['good']:\n",
    "        good = grouped_feats['good'][metric]\n",
    "        bad  = grouped_feats['bad'][metric]\n",
    "        if len(good) > 1 and len(bad) > 1:\n",
    "            t, p = ttest_ind(good, bad, equal_var=False)\n",
    "            print(f\"üìä {metric.upper()}\")\n",
    "            print(f\"   ‚û§ Good mean: {np.mean(good):.2f}\")\n",
    "            print(f\"   ‚û§ Bad  mean: {np.mean(bad):.2f}\")\n",
    "            print(f\"   ‚û§ t = {t:.2f}, p = {p:.4f}\\n\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipping {metric} due to insufficient data.\")\n",
    "\n",
    "compare_grouped_features(grouped_energy_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa571691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_grouped_feature_comparison(grouped_feats, figsize=(12, 6)):\n",
    "    metrics = list(grouped_feats['good'].keys())\n",
    "    metrics.sort()  # Sort to group same band together\n",
    "    x = np.arange(len(metrics))  # x-axis positions\n",
    "    \n",
    "    # Prepare means and std\n",
    "    good_means = [np.mean(grouped_feats['good'][m]) for m in metrics]\n",
    "    bad_means  = [np.mean(grouped_feats['bad'][m]) for m in metrics]\n",
    "    good_stds  = [np.std(grouped_feats['good'][m]) for m in metrics]\n",
    "    bad_stds   = [np.std(grouped_feats['bad'][m]) for m in metrics]\n",
    "\n",
    "    # Plot\n",
    "    width = 0.35\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.bar(x - width/2, good_means, width, yerr=good_stds, capsize=5, label='Good', color='skyblue')\n",
    "    ax.bar(x + width/2, bad_means, width, yerr=bad_stds, capsize=5, label='Bad', color='salmon')\n",
    "    \n",
    "    ax.set_ylabel('Feature Value')\n",
    "    ax.set_title('Energy Landscape Feature Comparison (Good vs Bad)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b1800",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grouped_feature_comparison(grouped_energy_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f0cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tda_metrics = [\n",
    "    'tda_entropy',        # Entropy of persistence diagram\n",
    "    'tda_betti0_max',     # Max number of connected components\n",
    "    'tda_betti1_max',     # Max number of 1D holes (loops)\n",
    "    'tda_pers1_mean',     # Avg lifetime of 1D holes\n",
    "    'tda_pers1_max',      # Max persistence of 1D holes\n",
    "    'tda_n_long_holes'    # Count of holes with lifetime > threshold\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f7845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ripser import ripser\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "import numpy as np\n",
    "\n",
    "# Step 4: Compute a distance matrix (Hamming or energy-weighted)\n",
    "def compute_distance_matrix(states, energies=None, use_energy_weighted=False):\n",
    "    \"\"\"\n",
    "    Compute a distance matrix for binary states.\n",
    "    If `use_energy_weighted` is True, weight distances by energy differences.\n",
    "    \"\"\"\n",
    "    hamming_distances = squareform(pdist(states, metric='hamming'))\n",
    "    if use_energy_weighted and energies is not None:\n",
    "        energy_weights = np.exp(-np.abs(energies[:, None] - energies[None, :]))\n",
    "        return hamming_distances * energy_weights\n",
    "    return hamming_distances\n",
    "\n",
    "# Step 5: Run persistent homology with ripser\n",
    "def run_persistent_homology(distance_matrix):\n",
    "    \"\"\"\n",
    "    Run persistent homology on the distance matrix using ripser.\n",
    "    \"\"\"\n",
    "    diagrams = ripser(distance_matrix, distance_matrix=True)['dgms']\n",
    "    return diagrams\n",
    "\n",
    "# Step 6: Extract TDA features\n",
    "def extract_tda_features(diagrams):\n",
    "    \"\"\"\n",
    "    Extract TDA features from persistence diagrams.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    # Persistence entropy\n",
    "    features['tda_entropy'] = entropy([d[1] - d[0] for d in diagrams[1] if d[1] < np.inf])\n",
    "    # Betti numbers\n",
    "    features['tda_betti0_max'] = len(diagrams[0])  # Number of connected components\n",
    "    features['tda_betti1_max'] = len(diagrams[1])  # Number of 1D holes (loops)\n",
    "    # Persistence statistics for 1D holes\n",
    "    lifetimes = [d[1] - d[0] for d in diagrams[1] if d[1] < np.inf]\n",
    "    features['tda_pers1_mean'] = np.mean(lifetimes) if lifetimes else 0\n",
    "    features['tda_pers1_max'] = np.max(lifetimes) if lifetimes else 0\n",
    "    features['tda_n_long_holes'] = sum(1 for l in lifetimes if l > 0.1)  # Threshold for \"long\" holes\n",
    "    return features\n",
    "\n",
    "# Step 7: Full TDA pipeline\n",
    "def tda_pipeline(patterns_dict, energies_dict=None, use_energy_weighted=False):\n",
    "    \"\"\"\n",
    "    Full TDA pipeline for all subjects and bands.\n",
    "    If energies_dict is not provided, compute energies using HopfieldNetwork.\n",
    "    \"\"\"\n",
    "    tda_features = {}\n",
    "    if energies_dict is None:\n",
    "        energies_dict = {}\n",
    "\n",
    "    for subj_id, bands in patterns_dict.items():\n",
    "        tda_features[subj_id] = {}\n",
    "        energies_dict[subj_id] = energies_dict.get(subj_id, {})\n",
    "        for band, states in bands.items():\n",
    "            try:\n",
    "                # Compute energies if not already done\n",
    "                if band not in energies_dict[subj_id]:\n",
    "                    model = HopfieldNetwork(n_units=states.shape[1])\n",
    "                    model.train_hebbian(states)\n",
    "                    energies = np.array([model.energy(state) for state in states])\n",
    "                    energies_dict[subj_id][band] = energies\n",
    "                else:\n",
    "                    energies = energies_dict[subj_id][band]\n",
    "                \n",
    "                # Compute distance matrix\n",
    "                distance_matrix = compute_distance_matrix(states, energies, use_energy_weighted)\n",
    "                \n",
    "                # Run persistent homology\n",
    "                diagrams = run_persistent_homology(distance_matrix)\n",
    "                \n",
    "                # Extract TDA features\n",
    "                tda_features[subj_id][band] = extract_tda_features(diagrams)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed for subject {subj_id}, band {band}: {e}\")\n",
    "                continue\n",
    "    return tda_features, energies_dict\n",
    "\n",
    "# Example usage\n",
    "# Assuming `patterns_rest` contains binary states\n",
    "tda_features_rest, energies_rest = tda_pipeline(patterns_rest, use_energy_weighted=True)\n",
    "\n",
    "# Compare features at the end\n",
    "def compare_tda_features(tda_features, good_ids, bad_ids):\n",
    "    \"\"\"\n",
    "    Compare TDA features between good and bad groups.\n",
    "    \"\"\"\n",
    "    grouped = {'good': defaultdict(list), 'bad': defaultdict(list)}\n",
    "    for group, ids in [('good', good_ids), ('bad', bad_ids)]:\n",
    "        for subj_id in ids:\n",
    "            subj_id = str(subj_id)\n",
    "            if subj_id not in tda_features:\n",
    "                continue\n",
    "            for band, features in tda_features[subj_id].items():\n",
    "                for metric, value in features.items():\n",
    "                    grouped[group][metric].append(value)\n",
    "    return grouped\n",
    "\n",
    "# Define good and bad groups\n",
    "good_ids = ['1', '2', '3', '5', '7', '8', '11', '12', '13', '15', '16', '17', '18', '20', '23', '24', '25', '26', '27', '28', '29', '31', '32', '33', '34', '35']\n",
    "bad_ids = ['0', '4', '6', '9', '10', '14', '19', '21', '22', '30']\n",
    "\n",
    "# Compare TDA features\n",
    "grouped_tda_features = compare_tda_features(tda_features_rest, good_ids, bad_ids)\n",
    "\n",
    "# Print comparison results\n",
    "for metric in all_tda_metrics:\n",
    "    good_values = grouped_tda_features['good'][metric]\n",
    "    bad_values = grouped_tda_features['bad'][metric]\n",
    "    if len(good_values) > 1 and len(bad_values) > 1:\n",
    "        t_stat, p_value = ttest_ind(good_values, bad_values, equal_var=False)\n",
    "        print(f\"üìä {metric.upper()}\")\n",
    "        print(f\"   ‚û§ Good mean: {np.mean(good_values):.2f}\")\n",
    "        print(f\"   ‚û§ Bad  mean: {np.mean(bad_values):.2f}\")\n",
    "        print(f\"   ‚û§ t = {t_stat:.2f}, p = {p_value:.4f}\\n\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Not enough data for {metric}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e47e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
